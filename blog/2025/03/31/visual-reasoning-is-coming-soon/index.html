
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="From silly cat costumes to world-changing innovations, OpenAI's latest release marks the beginning of something extraordinary. The fascinating world of visual reasoning is emerging, where AI models will soon think in pictures and solve complex spatial puzzles, transforming how machines understand and interact with the physical world.">
      
      
      
        <link rel="canonical" href="http://arcturus-labs.com/blog/2025/03/31/visual-reasoning-is-coming-soon/">
      
      
        <link rel="prev" href="../../../01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/">
      
      
        <link rel="next" href="../supercharging-llm-classifications-with-logprobs/">
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../../../assets/images/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.39">
    
    
      
        <title>Visual Reasoning is Coming Soon - Arcturus Labs</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.8c3ca2c6.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/custom.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-9ZGCYE3982"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-9ZGCYE3982",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-9ZGCYE3982",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  
  
  <meta property="og:title" content="Visual Reasoning is Coming Soon">
  <meta property="og:description" content="From silly cat costumes to world-changing innovations, OpenAI's latest release marks the beginning of something extraordinary. The fascinating world of visual reasoning is emerging, where AI models will soon think in pictures and solve complex spatial puzzles, transforming how machines understand and interact with the physical world.">
  <meta property="og:image" content="http://arcturus-labs.com/blog/assets/visual_reasoning_is_on_its_way/visual_reasoning_robot.jpg">
  <meta property="og:type" content="article">
  <meta property="og:url" content="http://arcturus-labs.com/blog/2025/03/31/visual-reasoning-is-coming-soon/">
  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Visual Reasoning is Coming Soon">
  <meta name="twitter:description" content="From silly cat costumes to world-changing innovations, OpenAI's latest release marks the beginning of something extraordinary. The fascinating world of visual reasoning is emerging, where AI models will soon think in pictures and solve complex spatial puzzles, transforming how machines understand and interact with the physical world.">
  <meta name="twitter:image" content="http://arcturus-labs.com/blog/assets/visual_reasoning_is_on_its_way/visual_reasoning_robot.jpg">
  

  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#why-image-manipulation-with-llms-stinks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Arcturus Labs" class="md-header__button md-logo" aria-label="Arcturus Labs" data-md-component="logo">
      
  <img src="../../../../../assets/images/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Arcturus Labs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Visual Reasoning is Coming Soon
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Arcturus Labs" class="md-nav__button md-logo" aria-label="Arcturus Labs" data-md-component="logo">
      
  <img src="../../../../../assets/images/logo.png" alt="logo">

    </a>
    Arcturus Labs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Blog
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Blog
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Blog
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Archive
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2026/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2026
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2025/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2025
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2024/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Categories
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Categories
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/agentic-ai/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agentic AI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/automation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Automation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Classification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/coding-patterns/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Coding Patterns
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/development-methodology/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Development Methodology
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/e-commerce/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    E-commerce
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/multimodal/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multimodal
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/prompt-engineering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt Engineering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/reasoning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reasoning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/retrieval/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Retrieval
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/user-experience/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    User Experience
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/vibe-coding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vibe Coding
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../contact/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contact
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#why-image-manipulation-with-llms-stinks" class="md-nav__link">
    <span class="md-ellipsis">
      Why Image Manipulation with LLMs Stinks
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#openais-release-brings-true-image-manipulation-to-llms" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI's Release Brings True Image Manipulation to LLMs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#visual-reasoning-will-be-the-real-innovation" class="md-nav__link">
    <span class="md-ellipsis">
      Visual Reasoning will be the Real Innovation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Visual Reasoning will be the Real Innovation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#demoing-visual-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      Demoing Visual Reasoning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-models-to-think-visually" class="md-nav__link">
    <span class="md-ellipsis">
      Training Models to Think Visually
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#from-chain-of-thought-to-reasoning-models" class="md-nav__link">
    <span class="md-ellipsis">
      From Chain-of-Thought to Reasoning Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#update-november-4th-2025" class="md-nav__link">
    <span class="md-ellipsis">
      Update November 4th, 2025
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#other-updates" class="md-nav__link">
    <span class="md-ellipsis">
      Other Updates
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
              <div class="md-post__authors md-typeset">
                
                  <div class="md-profile md-post__profile">
                    <span class="md-author md-author--long">
                      <img src="/assets/images/john_berryman_face.jpg" alt="John Berryman">
                    </span>
                    <span class="md-profile__description">
                      <strong>
                        
                          <a href="https://arcturus-labs.com/#about">John Berryman</a>
                        
                      </strong>
                      <br>
                      Founder of Arcturus Labs
                    </span>
                  </div>
                
              </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2025-03-31 00:00:00" class="md-ellipsis">March 31, 2025</time>
                      </div>
                    </li>
                    
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg>
                          <span class="md-ellipsis">
                            in
                            
                              <a href="../../../../category/reasoning/">Reasoning</a>, 
                              <a href="../../../../category/multimodal/">Multimodal</a></span>
                        </div>
                      </li>
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              14 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        


  <h1>Visual Reasoning is Coming Soon</h1>

<p>I gotta say – <em>I love it living in exponential times</em>. I can just wish that something existed and then within a month it does! This time it happened with <a href="https://openai.com/index/introducing-4o-image-generation/">OpenAI's 4o image generation release</a>. In this blog post I'll briefly cover the release and why I think it's pretty cool. Then I'll dive into a new opportunity that I think is even more exciting – <em>visual reasoning</em>.</p>
<p><img align="left" alt="Visual Reasoning Robot" src="../../../../assets/visual_reasoning_is_on_its_way/visual_reasoning_robot.jpg" width="100%" /></p>
<!-- more -->

<div class="admonition note">
<p class="admonition-title">Rather watch than read?</p>
<p>Hey, I get it - sometimes you just want to kick back and watch! Check out this quick video where I walk through everything in this post. Same great content, just easier on the eyes!</p>
<p><figure markdown="span">
  <iframe width="70%" src="https://www.youtube.com/embed/-xVKMqR4_f0" title="Visual Reasoning is Coming Soon" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure></p>
</div>
<h2 id="why-image-manipulation-with-llms-stinks">Why Image Manipulation with LLMs Stinks</h2>
<p>Working with images in Multimodal LLMs has been a mostly one-sided affair. On one hand, it's really cool that you can drop an image into an LLM conversation and get the model to reason about it. But when you ask the model to generate an image, there is a disconnect, because all the model can do is describe the image in text and then call out to an external image generation tool to generate the image based on that text. Text is a poor communication medium for images, and the resulting image is often quite disconnected from the expected result because the short description that the LLM provides to the image generation tool will rarely capture the full context of the conversation.</p>
<p>The problem is most pronounced when attempting to go back and forth working on an idea for an image. You can show the LLM an image of your cat and then say "make this cat wear a detective hat and a monocle". The best the model can do is to put a detective hat and monocle on <em>some</em> cat, not the one in your image. To make matters worse, the model can't even see the image that it has just created. So if you ask for a modification to first generation attempt, then the subsequent generations are really just starting over from scratch and hoping that a more detailed description to the image generation tool will make things better... it won't.</p>
<figure>
  <img align="left" alt="Your Cat" src="../../../../assets/visual_reasoning_is_on_its_way/cat.jpg" width="300px" />
  <img align="right" alt="Bad Cat Generation" src="../../../../assets/visual_reasoning_is_on_its_way/bad_cat.jpg" width="300px" />
<br />
<figcaption>Left: OpenAI's pet cat. Right: The best a traditional LLM can do when asked to give it a detective hat and monocle.</figcaption>
</figure>
<h2 id="openais-release-brings-true-image-manipulation-to-llms">OpenAI's Release Brings True Image Manipulation to LLMs</h2>
<p>OpenAI's image generation release changes all of this. OpenAI's newest GPT-4o model carries the full context of the conversation into the image generation, and rather than call out to an external model, it is the very same GPT-4o model that does the image generation. By carrying the full context of the conversation – <em>including the previous images</em> – into the generation of the new image you can generate much more consistent imagery.</p>
<p>Thus, stealing OpenAI's example, you can show it a photo of your cat, tell it to give <em>your cat</em> a detective hat, et voila! It works!</p>
<figure>
  <img align="left" alt="Your Cat" src="../../../../assets/visual_reasoning_is_on_its_way/cat.jpg" width="300px" />
  <img align="right" alt="Good Cat Generation" src="../../../../assets/visual_reasoning_is_on_its_way/good_cat.jpg" width="300px" />
<br />
<figcaption>Left: OpenAI's pet cat. Right: GPT-4o's generation when asked to give the same cat a detective hat and monocle. Much more consistent!</figcaption>
</figure>
<p>Now if all we could do with this technology is stick silly costumes on our cats, then this would honestly be a pretty lame achievement. But think about what we're angling for here. You'll be able to turn hand-drawn garbage into sophisticated infographics and marketing material. You'll be able to take crappy matplotlib charts and reformat them as professional-looking plots that go into slides. You'll be able to try on clothes virtually before you buy them. You'll be able to create photo collages from your photos, game assets, and 3D models. You'll be able to edit your vacation photos to remove that one person who blinked. You'll be able to take a photo of your living room and try out different furniture arrangements and color schemes. You'll be able to take a photo of your garden and get suggestions for where to plant new flowers. The possibilities are endless when you can manipulate images with the full context of a conversation.</p>
<h2 id="visual-reasoning-will-be-the-real-innovation">Visual Reasoning will be the Real Innovation</h2>
<p>All of the above is but prologue for what will come next. Models perform better when they take time to reason. This is because models have no internal monologue. Whereas humans pause and think about a question prior to answering, models have to generate tokens in order to think. But to this point models have only been able to reason in the space of words, and words are ill-suited for reasoning through many types of real world problems. But soon the models' new image manipulation capabilities will power a new form of reasoning. Right now, this ability is mostly useful for image editing. Soon, models will have the ability to visualize different scenarios and visually reason about the world around them.</p>
<h3 id="demoing-visual-reasoning">Demoing Visual Reasoning</h3>
<p>Let's try out an example right now and get a glimpse of how visual reasoning will work. For our experiment we'll use <a href="https://www.youtube.com/watch?v=aoikSxHXBYw&amp;t=546s">Matthew Berman's marble in a cup problem</a> – it's one of his go-to reasoning challenges when playing around with a new model:</p>
<blockquote>
<p>I've put a marble in a glass. The glass is then turned upside down and put on a table. Then the glass is picked up and put into the microwave. Where is the marble?</p>
</blockquote>
<p>It's a great problem because it requires common sense knowledge of how objects work in the world, and it's especially suited for this post because it requires spatial reasoning to extrapolate to a new scenario. I asked the new GPT-4o a variant of this problem in text:</p>
<blockquote>
<p>I set a plate on a table, and glass next to it. I set a marble on the plate. Then I pick up the marble, drop it in the glass. Then I turn the glass upside down and set it on the plate. Then, I pick up the glass and put it in the microwave. Where is the marble?</p>
</blockquote>
<p>I've added an extra item, a plate, for just a bit more complexity. Here's the answer it gave me:</p>
<blockquote>
<p>The marble is <strong>in the microwave, inside the glass.</strong></p>
<p>Here's a breakdown of what happened:</p>
<p>You placed a <strong>marble on the plate.</strong></p>
<p>You <strong>picked up the marble and dropped it in the glass</strong> — now the marble is <strong>in the glass.</strong></p>
<p>You <strong>turned the glass upside down and set it on the plate</strong> — unless the glass is sealed or the marble is somehow stuck, the marble would <strong>fall out onto the plate.</strong></p>
<p>Then you <strong>picked up the glass (now empty) and put it in the microwave.</strong></p>
<p>So at this point:</p>
<p>The <strong>glass is in the microwave.</strong></p>
<p>The <strong>marble is on the plate, outside</strong> the microwave.</p>
<p>But you asked: Where is the marble? You didn't say you picked it up again, so based on what you said, the marble is <strong>on the plate, not in the microwave.</strong></p>
<p>Want to run it back or add any twists?</p>
</blockquote>
<p>It's actually a pretty weird answer. It immediately answers – <em>incorrectly</em> – then performs post-hoc chain-of-thought reasoning and comes to the correct answer but ignores the fact that it completely contradicted itself.</p>
<p>Let's now do a vision-enhanced version of this chain-of-thought reasoning and demonstrate how spatial reasoning will improve the model's reasoning capabilities.</p>
<figure>
  <img align="left" alt="Problem Statement" src="../../../../assets/visual_reasoning_is_on_its_way/real_image.jpg" width="600px" />
<br />
<figcaption>Problem Statement Image</figcaption>
</figure>
<p>I prompted it with the following (including extra requirements to think visually):</p>
<blockquote>
<p>I'm going to pick up the marble, put it in the glass, turn the glass over onto the plate, pick the glass up, and then put the glass in the microwave. </p>
<p>The question is, where is the marble?</p>
<p>Solve this one step at a time using images to imagine what happens at each step. For each step, first state the step, and then generate an image corresponding to what the scene will look like after that step. And then summarize the image in English, highlighting the important aspects for solving the problem.</p>
</blockquote>
<p>Here are the reasoning responses from the model:</p>
<figure>
  <img align="left" alt="Step 1" src="../../../../assets/visual_reasoning_is_on_its_way/step1.jpg" width="600px" />
</figure>
<blockquote>
<p>Step 1: Pick up the marble.</p>
<p>In the image, the marble has not yet been moved, but we are about to pick it up and place it in the glass.</p>
</blockquote>
<figure>
  <img align="left" alt="Step 2" src="../../../../assets/visual_reasoning_is_on_its_way/step2.jpg" width="600px" />
</figure>
<blockquote>
<p>Step 2: Put the marble in the glass.</p>
<p>In the image, the marble is now inside the glass, and the plate is empty.</p>
</blockquote>
<figure>
  <img align="left" alt="Step 3" src="../../../../assets/visual_reasoning_is_on_its_way/step3.jpg" width="600px" />
</figure>
<blockquote>
<p>Step 3: Turn the glass over onto the plate.</p>
<p>In the image, the glass is now upside down on the plate, trapping the marble inside.</p>
</blockquote>
<figure>
  <img align="left" alt="Step 4" src="../../../../assets/visual_reasoning_is_on_its_way/step4.jpg" width="600px" />
</figure>
<blockquote>
<p>Step 4: Pick the glass up.</p>
<p>In the image, the hand is lifting the glass off the plate. The marble remains on the plate, as it is no longer contained.</p>
</blockquote>
<figure>
  <img align="left" alt="Step 5" src="../../../../assets/visual_reasoning_is_on_its_way/step5.jpg" width="600px" />
</figure>
<blockquote>
<p>Step 5: Put the glass in the microwave.</p>
<p>In the image, the glass is now inside the microwave with the marble at the bottom.</p>
<p>Final answer: The marble is inside the microwave, at the bottom of the glass.</p>
</blockquote>
<p>So... uh... notably, we made some amazingly consistent images, and then <em>completely</em> got the answer wrong. And we were so close! At Step 4, the model acknowledged that the marble was no longer contained in the glass. Unfortunately, by the time the glass was in the microwave, the model imagined that the marble had somehow followed it.</p>
<h3 id="training-models-to-think-visually">Training Models to Think Visually</h3>
<p>I think the failure to correctly use visual reasoning is because the model hasn't yet been <em>trained</em> for visual reasoning.</p>
<p>Who knows how OpenAI trained these models, but based on their <a href="https://openai.com/index/introducing-4o-image-generation/">debuting post</a>, the training to date is almost certainly unrelated to visual reasoning. The examples in that post – while extremely impressive – are about better rendering for text, and better instruction following. But all the instructions follow examples related to image manipulation rather than reasoning. Instead, we need to start training models <em>specifically</em> to perform visual reasoning.</p>
<p>For visual reasoning practice, we can do supervised fine-tuning on sequences similar to the marble example above. For instance, to understand more about the physical world, we can show the model sequential pictures of Slinkys going down stairs, or basketball players shooting 3-pointers, or people hammering birdhouses together. We can make the model become more socially aware by letting it see sequences of text and images from social interactions, and having it predict body language and facial expressions for the next scene. We can train for spatial reasoning by having models solve tangrams or having them visualize what a contraption will look like when rotated by 90 degrees. If you think for very long it's easy to can come up with tons of ideas like these – visual reasoning is central to most of our lives.</p>
<p>But where will we get all this training data? For spatial and physical reasoning tasks, we can leverage computer graphics to generate synthetic data. This approach is particularly valuable because simulations provide a controlled environment where we can create scenarios with known outcomes, making it easy to verify the model's predictions. But we'll also need real-world examples. Fortunately, there's an abundance of video content online that we can tap into. While initial datasets might require human annotation, soon models themselves will be able to process videos and their transcripts to extract training examples automatically. Consider all the how-to videos online - creators typically explain what they're going to do before demonstrating it. We can use these to create training pairs: given the current scene and verbal description, have models predict what happens next.</p>
<p>As an aside – I think that videos are going to be our richest source of new training data, and we're just now starting to really tap into it. But who owns the all of the video content on the web? YouTube... Google. I'd say that bodes really well for the future of the Gemini models. Wouldn't you?</p>
<h3 id="from-chain-of-thought-to-reasoning-models">From Chain-of-Thought to Reasoning Models</h3>
<p><a href="https://arxiv.org/abs/2201.11903">As early as 2022</a> we had recognized that LLMs profited from chain-of-thought reasoning. If a model was making a snap judgment and rushing to an incorrect solution, it was commonplace to add a "let's think step by step", hope for a better output, and often get it. Soon, deeper reasoning approaches were discovered. In particular, <a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts</a> encouraged the model to think through multiple possible approaches for solving a problem, rank them according to perceived feasibility, and then the application would help direct the model optimally through the paths and toward the problem solution.</p>
<p>With OpenAI's o1 model and several models that followed soon thereafter, the ability to do this sort of tree-of-thoughts reasoning is baked into the model. Prior to providing a final solution, the model may now think through the problem within special "thinking" tags. According to DeepSeek (who has been much more transparent than OpenAI), their R1 model has been trained through a form of reinforcement learning with verifiable problems so that the model can be rewarded for efficiently thinking its way to the <em>verifiable</em> correct solution. Interestingly, and reminiscent of tree-of-thoughts, the models can think themselves into a corner – <em>recognize that they have messed up</em> – backtrack, and get back to a better course toward solving the problem.</p>
<p>Enter visual reasoning. In the coming year, models will undergo supervised fine-tuning to develop scene prediction abilities - given a current scene and a proposed change, they'll learn to visualize the outcome. For example, they'll be able to mentally rotate objects in 3D space or anticipate how people might react in social situations. As training progresses using scenarios with verifiable outcomes, models will develop more sophisticated reasoning capabilities. They'll be able to observe their environment, formulate plans, mentally simulate the consequences of different actions, and learn from comparing their predictions to real-world results. This advancement will be particularly transformative for robotics, where physical interaction with the environment requires robust visual understanding and planning.</p>
<p>Currently, there is one obvious problem with visual reasoning – image creation is a very slow process. However, like everything else in our world right now, this will soon become faster and more feasible. But even if the visual processing remains slow, the very act of training the models to reason about images will improve the text reasoning for visual/spatial problems. What's more, the internal representation of the images seems to be created at several levels of granularity – a small blurry image, and then a refined high-def image. The image you see when playing with ChatGPT is the latter, high-def image. Perhaps visual reasoning can make use of the blurry image – it will be quicker to generate and will help with the reasoning process.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I'm excited about what's coming! Just like every moment for the past 4 years, we're on the cusp of a radical discovery that will change our world – a radical discovery which is soon realized. I don't think this one is going to let us down. The ever-more capable <em>visual</em> reasoning models will be able to make better sense of our work – not only in terms of understanding the mechanics of physical objects, but also in reading social cues, and really in anything else that we do where vision is of use to us! And then 2026 will be the year of the robots...</p>
<figure>
  <img align="left" alt="Terminator" src="../../../../assets/visual_reasoning_is_on_its_way/terminator.jpg" width="600px" />
</figure>
<hr />
<h2 id="update-november-4th-2025">Update November 4th, 2025</h2>
<p>I just got access to Sora 2 for video generation, and it's spectacular! So – hopeful – I attempted to recreate my broken experiment above to see if visual reasoning was now the state of the art ... and ... <em>maybe not quite yet.</em> </p>
<p>Here are my two attempts (both of which also included the first picture above of the plate, the glass, and the marble):</p>
<blockquote>
<p>I'm going to pick up the marble, put it in the glass, turn the glass over onto the plate, pick the glass up, and then put the glass in the microwave. The question is, where is the marble? Perform of each of these steps and state aloud what you’re doing each time and then when you’re finished, tell us exactly where the marble is.</p>
</blockquote>
<p>This lead to <a href="https://sora.chatgpt.com/p/s_690a1734ab6c8191ae268a498fd0d194">this video</a>.</p>
<p>After this I tried a more careful prompt - which lead to worse results.</p>
<blockquote>
<p>For the following experiment, you have a plate, a glass, and a marble. Perform the following the experiment, in a single take with no cuts: Pick up the marble and drop it in the glass. In one motion turn the glass over onto the plate upside down. Then pick the glass up, and then put the glass in the microwave. As you are doing all of this narrate each step out loud. The question is, where is the marble? Once you are finished with the above steps, tell the viewers where the marble is. And then pan and zoom the camera to focus upon the marble.</p>
</blockquote>
<p>This prompt lead to <a href="https://sora.chatgpt.com/p/s_690a2682a6248191bd727aaa9b44ee76">this generated video</a>.</p>
<p>In both cases the request was disobeyed, and the objects referred to verbally by the narrator were not present on the screen! Curiously though, in both videos, the narrator reached the same correct conclusion despite none of video making sense. I wonder how many times the AI would reach the correct conclusion even though the video didn't portray it. This might indicate that there is some sort of semantic reasoning happening in the "old" LLM portion of the model, and that reasoning is being conveyed into the video (because it at least is attempting to follow the process as I requested). But then the information from the video doesn't seem as tightly bound to the semantics. It's consistent with the initial prompt, and it's <em>mostly</em> self-consistent, but the semantics portions of the model don't appear to be noticing the facts of what it "sees with its eyes".</p>
<p>Nevertheless, I'm still going to keep an eye on this space. I think over the next year or two, we're going to see AI become "embodied" into physical system that work with us, and this kind of visual pre-simulation is going to be important for helping them to navigate their world. I suspect it won't be visual in the exact same sense as generating a high-quality video, but it will be something analogous to this, perhaps in a lower resolution space or even a more abstract embedding space that still captures the spatially salient aspects. And I think we'll do well to train the models to attend more to the visuals that they are producing so that they remain consistent and moored to the semantics, and actually inform the semantic layer – after all, that's what visual reasoning is.</p>
<h2 id="other-updates">Other Updates</h2>
<ul>
<li>2025.11.11 - <a href="https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence?utm_source=substack&amp;utm_medium=email">From Words to Worlds: Spatial Intelligence is AI’s Next Frontier</a> – Fei-Fei Li's essay on the importance of spatial reasoning and how to get there.</li>
</ul>







  
  



  


  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["content.code.copy"], "search": "../../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.525ec568.min.js"></script>
      
        <script src="../../../../../assets/scripts/custom.js"></script>
      
    
  </body>
</html>