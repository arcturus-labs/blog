{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"fulfillment-policy/","title":"Fulfillment Policy","text":"<p>Last Updated: June 2025</p> <p>At Arcturus Labs, we provide high-quality, customized AI consulting services. Our offerings include strategic assessments, training, and prototyping, all tailored to the client\u2019s needs and delivered remotely, asynchronously, or in person. This policy outlines how engagements are fulfilled, and how cancellations and refunds are handled.</p>"},{"location":"fulfillment-policy/#1-service-delivery","title":"1. Service Delivery","text":"<p>All services are delivered under individually negotiated contracts. Each agreement includes a clear description of the engagement process, deliverables, and timelines. Clients are provided with this information before work begins.</p>"},{"location":"fulfillment-policy/#2-refund-policy","title":"2. Refund Policy","text":"<ul> <li>Clients may cancel with at least seven (7) calendar days\u2019 notice prior to the scheduled start of work for an invoicing period. In such cases, the associated invoice will be canceled and no work will be performed.</li> <li>If a client cancels after an invoice has been paid, no refund will be issued except in exigent circumstances (serious illness, natural disaster, or similar unforeseen hardship). In such cases, a partial refund may be considered for undelivered work or unused time.</li> <li>If Arcturus Labs must cancel a project, a full refund will be issued for any undelivered services.</li> <li>If a client is dissatisfied with the results, we do not offer refunds, but we typically provide a course correction or follow-up session to address concerns.</li> </ul>"},{"location":"fulfillment-policy/#3-cancellations-rescheduling","title":"3. Cancellations &amp; Rescheduling","text":"<p>The same cancellation policy applies to both project-based and retainer-based work. Clients may cancel with at least seven (7) calendar days\u2019 notice prior to the scheduled start of work for an invoicing period. Rescheduling of individual sessions requires reasonable notice (24-48 hours) and will be accommodated as availability allows.</p>"},{"location":"fulfillment-policy/#4-contact","title":"4. Contact","text":"<p>Clients may contact us via email with any questions or concerns related to service delivery: john@arcturus-labs.com</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/11/21/roaming-rag--rag-without-the-vector-database/","title":"Roaming RAG \u2013 RAG without the Vector Database","text":"<p>Let's face it, RAG can be a big pain to set up, and even more of a pain to get right.</p> <p>There's a lot of moving parts. First you have to set up retrieval infrastructure. This typically means setting up a vector database, and building a pipeline to ingest the documents, chunk them, convert them to vectors, and index them. In the LLM application, you have to pull in the appropriate snippets from documentation and present them in the prompt so that they make sense to the model. And things can go wrong. If the assistant isn't providing sensible answers, you've got to figure out if it's the fault of the prompt, the chunking, or the embedding model.</p> <p>If your RAG application is serving documentation, then there might be an easy alternative. Rather than setting up a traditional RAG pipeline, put the LLM assistant to work. Let it navigate through the documentation and find the answers. I call this \"Roaming\" RAG, and in this post I'll show you how it's done.</p> <p></p> <p>Blog posts not your thing?</p> <p>Not in the mood for reading? Join me, John Berryman, with my velvety radio voice, as I guide you through this fascinating topic in just 10 minutes! Grab your favorite snack, settle in, and let's explore Roaming RAG together.</p> <p> </p>"},{"location":"blog/2024/11/21/roaming-rag--rag-without-the-vector-database/#the-big-idea","title":"The Big Idea","text":"<p>Back in the olden days when the internet was a baby, we didn't have search engines. Instead, companies like Yahoo and Excite attempted to organize the internet into a directory structure. If you wanted to ask the internet how to change your car oil, you would peruse the directory of everything, select the Auto section, followed by Maintenance, and then finally the Oil Change section. Then you look at whatever website was lucky enough to be listed in order to figure out how to change your oil. This system worked horribly.</p> <p>However, if you are looking for answers in a relatively small set of well-organized documents, then a hierarchical directory might still be a pretty good approach. For example, when you're reading a tech reference book, how do you find the information you need? You probably open up to the table of contents, then review the chapters and sections that might be useful, and then find the content you're looking for.</p> <p>It turns out that there are plenty of examples of content that are well-organized and amenable to this type of navigational search. Just to name a few:</p> <ul> <li>Legal code which organizes the laws into a hierarchy of title, chapter, section, and paragraph.</li> <li>Technical books organized into parts, chapters, sections, subsections, and paragraphs.</li> <li>Product owner\u2019s manuals structured into sections and topics.</li> <li>Curriculums or syllabuses dividing educational content into course overview, units, lessons, and assignments.</li> <li>Corporate policy manuals structured by policy area, individual policies, and procedures.</li> </ul> <p>The big idea of Roaming RAG is to craft a simple LLM application so that the LLM assistant is able to read a hierarchical outline of a document, and then rummage though the document (by opening sections) until it finds and answer to the question at hand. Since Roaming RAG directly navigates the text of the document, there is no need to set up retrieval infrastructure, and fewer moving parts means less things you can screw up!</p>"},{"location":"blog/2024/11/21/roaming-rag--rag-without-the-vector-database/#demo","title":"Demo","text":"<p>In order to demo Roaming RAG, we'll be looking at llms.txt files because they are typically good examples of the well-structured documents I'm talking about. ... Wait, you don't know what llms.txt is? Well it's only the new hotness!</p> <p>What is llms.txt?</p> <p>Jeremey Howard proposed llms.txt (here) as a machine-readable document that is associated with a website. Similar to /sitemap.xml and /robots.txt, llms.txt is intended to be read by machines, and it serves as a structured guide to help large language models quickly understand the key information about a website or project. This will give LLM-based assistants the ability to do all sorts of neat things \u2013 like answer questions about someone's public CV page, describe courses offered at educational institution, or provide programmatic examples for a software library.</p> <p>The most important thing for our purposes is that llms.txt is formatted as a well-structured markdown document \u2013 something that is both easy for LLMs to consume and easy for me to use in a demo.</p> <p>Here is a page that lists a growing set of websites that are adopting the llms.txt standard.</p> <p>For our example, we will use Cursor's llms.txt (take look here) which is relatively well organized and does a good job describing their product.</p> <p>In this example, we load the Cursor llms.txt into an artifact labeled \"Abridged llms.txt\", which is also presented to the model (see my previous post for an intro to artifacts \u2013 they are not critical for this post).  Notice that the sections of the llms.txt have been truncated. This greatly reduces the size of the llms.txt. However, the section headings and opening text is preserved \u2013 this ensures that the assistant can still understand and navigate the document.</p> <p>In the demo, the user asks a simple question, \"How does Cursor's Tab autocomplete feature differ from GitHub Copilot?\" The assistant then roams through the document and expands the sections it deems useful in answering the user's question. These expanded section are presented to the model as additional artifacts, which are also presented in new tabs in the artifact pane \u2013 in this case \"Overview\" and \"Migrate from GitHub Copilot\". Once the assistant has sufficient information to address the user's question, it responds in the conversation tab with a grounded and relevant answer.</p> <p>Now You Try!</p> <p>Try the demo yourself here. When you load the window you'll be able to select from a large number of llms.txt documents from the /llms.txt directory. In the bottom right there are several LLM-generated example questions that you can ask \u2013 but I encourage you to ask your own questions and see how well it works for you.</p>"},{"location":"blog/2024/11/21/roaming-rag--rag-without-the-vector-database/#how-does-it-work","title":"How Does It Work?","text":"<p>The idea is pretty simple \u2013 just present the model with an abridged version of the document (or documents) that it will navigate, and then give it the tools and know-how required to navigate the document and find information that the user is looking for.</p>"},{"location":"blog/2024/11/21/roaming-rag--rag-without-the-vector-database/#prerequisite-well-organized-documentation","title":"Prerequisite - Well-Organized Documentation","text":"<p>The whole premise of this approach is that we let the assistant look at an outline of the document and navigate it as it sees fit. This means that this approach only works well for documents that are well-organized. This means:</p> <ul> <li>Titles and headings should be clear and self-explanatory.</li> <li>Each section has opening text that further clarifies the contents of the section.</li> <li>Each section of the document should only contain subsections that logically fit within the topic of the section.</li> <li>There shouldn't be tons of top-level sections.</li> </ul> <p>So again, we're talking about technical documents, catalogs, well-organized websites (and ideally the llms.txt that describes them). We're not talking about novels or pages of disorganized facts.</p>"},{"location":"blog/2024/11/21/roaming-rag--rag-without-the-vector-database/#preparing-the-abridged-document","title":"Preparing the Abridged Document","text":"<p>As soon as the Roaming RAG application loads the document, it parses the text into sections and subsections, and it produces an abridged version of the text like this one taken from Cursor's llms.txt:</p> <pre><code># AI Review (Beta) &lt;!-- Section collapsed - expand with expand_section(\"07f3277d\") --&gt;\n\nAI Review is a feature that allows you to review your recent changes in your codebase to catch any potential...\n\n### Custom Review Instructions... &lt;!-- Section collapsed - expand with expand_section(\"f5cc6b18\") --&gt;\n\n### Review Options... &lt;!-- Section collapsed - expand with expand_section(\"aaa9c5c8\") --&gt;\n\n\n\n# Custom API Keys &lt;!-- Section collapsed - expand with expand_section(\"919b9b9a\") --&gt;\n\n...\n\n### OpenAI API Keys... &lt;!-- Section collapsed - expand with expand_section(\"9db61152\") --&gt;\n\n### Anthropic API Keys... &lt;!-- Section collapsed - expand with expand_section(\"13471f04\") --&gt;\n\n### Google API Keys... &lt;!-- Section collapsed - expand with expand_section(\"25ff5a5c\") --&gt;\n\n### Azure Integration... &lt;!-- Section collapsed - expand with expand_section(\"d851e3fd\") --&gt;\n\n### Will my API key be stored or leave my device?... &lt;!-- Section collapsed - expand with expand_section(\"0da6eab3\") --&gt;\n\n\n\n# Models &lt;!-- Section collapsed - expand with expand_section(\"a8a96034\") --&gt;\n\nWith Cursor Chat, Ctrl/\u2318 K, and Terminal Ctrl/\u2318 K, you can easily switch between different models of your...\n\n### Model Dropdown... &lt;!-- Section collapsed - expand with expand_section(\"b9e8ef5c\") --&gt;\n\n### Long Context Only Models... &lt;!-- Section collapsed - expand with expand_section(\"11304f12\") --&gt;\n\n### What context window is used for model X?... &lt;!-- Section collapsed - expand with expand_section(\"f1d4f327\") --&gt;\n\n[...]\n</code></pre> <p>(Again, compare this with Cursor's full llms.txt doc.)</p> <p>Here are the things to notice:</p> <ul> <li>We keep the top-level headings.</li> <li>We preserve the first ~100 characters of opening text from each section.</li> <li>We present the second-level headings, but without any subsidiary content.</li> <li>Each section is provided a unique 8 digit hex identifier.</li> <li>Each section heading is followed by a guiding comment for the model: <code>Section collapsed - expand with expand_section(\"{identifier}\")</code>.</li> </ul> <p>Splitting up the document like this is not terribly difficult. In these 300 or so lines of code, I'm using <code>markdown_it</code> to split the doc into sections, give them all a unique identifier, and make it possible to easily access and expand subsections using their identifier.</p>"},{"location":"blog/2024/11/21/roaming-rag--rag-without-the-vector-database/#backend-implementation","title":"Backend Implementation","text":"<p>As our guiding comment implies, we have a tool named <code>expand_section</code>. It's pretty simple:</p> <pre><code>{\n    \"name\": \"expand_section\", \n    \"description\": \"Expand a section of the markdown document to reveal its contents.\\n\\n\"\n                  \"- Expand the most specific (lowest-level) relevant section first\\n\"\n                  \"- Multiple sections can be expanded in parallel\\n\"\n                  \"- You can expand any section regardless of parent section state (e.g. parent sections do not need to be expanded to view subsection content)\\n\"\n    \"input_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"section_id\": {\n                \"type\": \"string\",\n                \"description\": \"8-digit hexadecimal ID of the section to expand (e.g. '04c8214b')\",\n                \"pattern\": \"^[0-9a-f]{8}$\"\n            }\n        },\n        \"required\": [\"section_id\"]\n    }\n}\n</code></pre> <p>If the model calls <code>expand_section</code> then the corresponding section of the document is retrieved and presented to the model in another artifact. (The artifact is also presented to the user as a new tab in the artifact pane. Using artifacts isn't required for Roaming RAG, but it was a convenient choice coming from my last blog post, and it seemed to work well here too.)</p> <p>The final touch is to create a system message to explain that this assistant's task is to help answer questions from the associated llms.txt. I explain the idea of llms.txt, and the manner in which the assistant is to navigate the document and answer questions. (Here's the code if you're interested.).</p>"},{"location":"blog/2024/11/21/roaming-rag--rag-without-the-vector-database/#in-action","title":"In Action","text":"<p>Upon reading the user's question, the assistant \"roams\" around the document by glancing over the outline and opening sections that seem to be relevant. There are a few tricks that it can do:</p> <ul> <li>The assistant might open several sections at the same level as it looks for content that satisfies the user's request.</li> <li>The assistant can dig deeper into the text by opening a section and then reading the new content within that section and choosing to open a deeper subsection.</li> <li>Because the abridged document shows two levels of unopened sections, the assistant can optionally skip a level of detail and dive two sections deep.</li> </ul> <p>There are actually a couple of tricks that I left out. For some reason I didn't design the <code>expand_section</code> tool to take a list of ids. This would have allowed the assistant to open up multiple sections in parallel and thus saved a bit of time. I did do some experimentation with a <code>collapse_section</code> command that allowed the assistant clean up open sections that were no longer relevant to the conversation. Unfortunately, the assistant would either never use that tool, or, if I insisted that the tool be used, then the agent would use it indiscriminately and at the wrong time. I instead opted to just close all open sections at the beginning of a question. This is a shame, because if the user has follow-up questions about a topic it would be better to keep the sections open. ... Oh well, next time.</p> <p>There are times when Roaming RAG is unable to find the information it needs. Likely this is because the document isn't well-structured, or perhaps the headings aren't descriptive enough for the model to make sense of them. But, sometimes the model could also just overlook the right section. When this happens, I've prompted the model to apologize to the user and recommend other ideas for tracking down their answer.</p>"},{"location":"blog/2024/11/21/roaming-rag--rag-without-the-vector-database/#conclusion","title":"Conclusion","text":"<p>One thing to underscore is that Roaming RAG isn't a drop-in replacement for your traditional RAG application. Roaming RAG only works when the docs in question are well-organized as described above. There are also costs to consider \u2013 Roaming RAG might make several lookups before it finds the information it needs, and the prompt can be long \u2013 prompt caching is advised.</p> <p>But if you find yourself with a well organized document \u2013 and llms.txt is a great example of this \u2013 then Roaming RAG offers some nice benefits.</p> <p>One benefit is the richer context. In traditional RAG, context is retrieved as chunks of text which, when shoved into the prompt, resemble pages ripped out of a book. With Roaming RAG, the information retrieved is always presented within the context of the surrounding document. Intuitively, this will likely help the model build a better-informed response to the user's question.</p> <p>The other main benefit is that there is no extra infrastructure to set up for Roaming RAG \u2013 no need to chunk documents, vectorize, or store them in a vector database \u2013 actually, no need for the vector database at all. The implementation just needs the doc itself and about 300 lines of code to parse it and get everything set up.</p> <p>Special thanks to Juan Pablo Mesa Lopez and Dan Becker for providing feedback on this post.</p>"},{"location":"blog/2024/11/21/roaming-rag--rag-without-the-vector-database/#hey-and-if-you-liked-this-post-then-maybe-we-should-be-friends","title":"Hey, and if you liked this post, then maybe we should be friends!","text":"<ul> <li>I just wrote a book about Prompt Engineering for LLM Applications. Maybe you'd be interested in reading it.</li> <li>Are you stumped on a problem with your own LLM application? Let me hear about it.</li> <li>I'm going to write lots more posts. Subscribe and you'll be the first to know.</li> </ul>"},{"location":"blog/2024/10/09/bridging-the-gap-between-keyword-and-semantic-search-with-splade/","title":"Bridging the Gap Between Keyword and Semantic Search with SPLADE","text":"<p>In information retrieval, we often find ourselves between two tools: keyword search and semantic search. Each has strengths and limitations. What if we could combine the best of both?</p> <p>By the end of this post, you will:</p> <ul> <li>Understand the challenges of keyword and semantic search</li> <li>Learn about SPLADE, an approach that bridges these methods</li> <li>See a practical implementation of SPLADE to enhance search</li> </ul> <p>If you've struggled with inaccurate search results or wanted a more transparent search system, this post is for you. Let's explore how SPLADE can change your approach to information retrieval.</p>"},{"location":"blog/2024/10/09/bridging-the-gap-between-keyword-and-semantic-search-with-splade/#the-unfortunate-state-of-the-art","title":"The Unfortunate State of the Art","text":"<p>With the rise of RAG methods in prompt engineering, vector-based semantic search has become essential for many applications. It's easy to see why: semantic search overcomes some key limitations of keyword search. In traditional keyword search, you might type terms that mean the same thing as the document you're seeking, but if you use different words, you won't get a match. For example, searching for \"ape costume\" won't find a document mentioning \"gorilla suit.\" Semantic search, on the other hand, converts your query into a vector representing its meaning. If there's a document with a similar meaning (represented by a nearby vector), you get a match!</p> <p>Semantic search seems almost magical... until it's not.</p> <p>There are some gnarly challenges with semantic search that we're still grappling with:</p> <ul> <li>Larger indexes \u2013 Keyword search indexes typically grow to 1.5x-2x the original document size. Semantic search indexes can be twice that size.</li> <li>Chunking complexity \u2013 You need to split text into chunks because embedding quality degrades with too much input. But where do you split? Do chunks need to overlap? How do you ensure important context isn't lost?</li> <li>Lack of transparency \u2013 With keyword search, debugging is straightforward \u2013 the tokens are human-readable, so you can understand why a document matches. You can adjust queries, field boosts, and phrase matches to tune relevance. Semantic search is opaque; if a query doesn't match as expected, it's hard to understand why. Fixing relevance often means training a new embedding model and reindexing everything. Ouch!</li> </ul> <p>Wouldn't it be great to have the best of both worlds? We want semantic search's ability to match on meaning, combined with the transparency and simplicity of traditional keyword search.</p>"},{"location":"blog/2024/10/09/bridging-the-gap-between-keyword-and-semantic-search-with-splade/#enter-splade","title":"Enter SPLADE","text":"<p>SPLADE (Sparse Lexical and Expansion Model for First Stage Ranking) was introduced in a July 2021 paper and quickly improved upon in a September follow-up. The concept is simple: instead of asking a semantic model for a meaning-carrying vector, ask it for important terms that should be in the document, whether they're actually present or not. For instance, given a document containing \"ape costume,\" the model might identify similar terms like \"gorilla orangutan monkey suit clothes.\" These synthetic terms can then be indexed in a traditional search engine, boosting recall when added to the search field.</p> <p>In this post, we'll explore how to use SPLADE to enhance your search. We'll create a silly document set (because what fun is it to use a realistic example?), index it, and demonstrate how conventional search can fall short when query terms don't quite match. Then, we'll add SPLADE and show how it addresses this problem.</p>"},{"location":"blog/2024/10/09/bridging-the-gap-between-keyword-and-semantic-search-with-splade/#setup","title":"Setup","text":"<p>What's your favorite superhero? Superman? Wolverine? Batman? ... Mine's got to be Hindsight Lad \u2013 a computer researcher who contributed to his team by critically reviewing past decisions and explaining what they should have done instead. (Real character! Look him up!)</p> Image borrowed from the Marvel fandom wiki <p>Inspired by Hindsight Lad, I've chosen superheroes for our example dataset. It's a simple list of superheroes including their names, true identities, descriptions, and superpowers. Here's an excerpt:</p> Name True Identity Description Superpowers Spider-Man Peter Parker A high school student bitten by a radioactive spider Web-slinging, superhuman strength, spider-sense Hindsight Lad Carlton LaFroyge A teenager with the ability to analyze past events and point out mistakes Retroactive clairvoyance, tactical analysis of past events Batman Bruce Wayne A billionaire industrialist and philanthropist Genius-level intellect, master detective, peak human physical condition Arm-Fall-Off Boy Floyd Belkin A superhero with the ability to detach his arms Detachable arms, using detached arms as weapons (yes... another real character!) Superman Clark Kent An alien from the planet Krypton Flight, super strength, heat vision, invulnerability <p>To demonstrate the semantic mismatch problem, I've also generated alternative descriptions that convey the same meaning but use almost no common words:</p> Name Alternate Description Spider-Man An adolescent scholar affected by an irradiated arachnid Hindsight Lad A young critic gifted with retrospective wisdom Batman A wealthy entrepreneur and humanitarian Arm-Fall-Off Boy A costumed vigilante capable of limb separation Superman An extraterrestrial being from a distant celestial body <p>Our curated list has just 50 heroes, so querying with alternate descriptions might work well for semantic search, but traditional information retrieval will likely struggle.</p>"},{"location":"blog/2024/10/09/bridging-the-gap-between-keyword-and-semantic-search-with-splade/#indexing","title":"Indexing","text":"<p>Let's demonstrate this. Here is function that will index all of our documents:</p> <pre><code>def index_superheroes(num_tokens=50):\n    # Create the index with mappings\n    index_name = \"superheroes\"\n    mappings = {\n        \"mappings\": {\n            \"dynamic\": \"false\",\n            \"properties\": {\n                \"description\": {\n                    \"type\": \"text\",\n                    \"analyzer\": \"english\",\n                },\n                \"splade\": {\n                    \"type\": \"text\",\n                }\n            }\n        }\n    }\n\n    # delete and recreate the index\n    if es.indices.exists(index=index_name):\n        es.indices.delete(index=index_name)\n        print(f\"Index '{index_name}' deleted successfully.\")\n    else:\n        print(f\"Index '{index_name}' does not exist.\")\n\n    es.indices.create(index=index_name, body=mappings)\n    print(f\"Index '{index_name}' created successfully.\")\n\n    df = pd.read_csv('superheroes.csv')\n    # Index the superheroes\n    for i, (index, row) in enumerate(df.iterrows(), start=1):\n        # Combine the index (superhero name) with the row data\n        full_row = pd.concat([pd.Series({'name': index}), row])\n        doc = full_row.to_dict()\n        doc['splade'] = get_splade_embedding(doc['description'], num_tokens)\n        es.index(index=index_name, id=i, body=doc)\n\n    print(f\"Indexed {len(df)} superheroes.\")\n</code></pre> <p>This script creates an index with two fields: <code>description</code> for superhero descriptions and <code>splade</code> for synthetic terms. The SPLADE content is generated by processing the <code>description</code> through <code>get_splade_embedding</code>, which we'll define next:</p> <pre><code>from transformers import AutoModelForMaskedLM, AutoTokenizer\nimport torch\n\n# Load the SPLADE model and tokenizer\nmodel_id = 'naver/splade-cocondenser-ensembledistil'\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForMaskedLM.from_pretrained(model_id)\n\n# Create a mapping from token IDs to tokens\nvocab = tokenizer.get_vocab()\nid2token = {v: k for k, v in vocab.items()}\n\ndef get_splade_embedding(text, num_tokens=50):\n    # get the tokens\n    tokens = tokenizer(text, return_tensors='pt')\n\n    # get the splade embedding\n    output = model(**tokens)\n    vec = torch.max(\n        torch.log(\n            1 + torch.relu(output.logits)\n        ) * tokens.attention_mask.unsqueeze(-1),\n    dim=1)[0].squeeze()\n\n    # Convert vec to numpy for easier manipulation\n    vec_np = vec.detach().numpy()\n\n    # Get indices of non-zero elements\n    non_zero_indices = vec_np.nonzero()[0]\n\n    # Create a list of (token, value) pairs for non-zero elements, excluding the input tokens\n    token_value_pairs = [\n        (id2token[idx], vec_np[idx]) \n        for idx in non_zero_indices \n        if idx not in tokens['input_ids'][0]\n    ]\n\n    # Sort by value in descending order\n    token_value_pairs.sort(key=lambda x: x[1], reverse=True)\n\n    new_tokens = [token for token, value in token_value_pairs[:num_tokens]]\n\n    return new_tokens\n</code></pre> <p>This code is more complex, but builds on existing work. It's adapted from Pinecone's SPLADE writeup, with equations detailed in the SPLADEv2 paper. Essentially, it extracts tokens from input text, uses the SPLADE model to identify important terms (SPLADE tokens), filters out original tokens, converts remaining tokens to readable text, and returns the result.</p>"},{"location":"blog/2024/10/09/bridging-the-gap-between-keyword-and-semantic-search-with-splade/#searching","title":"Searching","text":"<p>What good is an index that can't be searched? Let's remedy that:</p> <pre><code>def search_superheroes(description, size, splade):\n    # If SPLADE is enabled, we search both the description and SPLADE fields\n    if splade:\n        # Get SPLADE tokens for the description\n        splade_tokens = get_tokens_as_text(description)\n        query = {\n            \"query\": {\n                \"bool\": {\n                    \"should\": [\n                        {\n                            \"multi_match\": {\n                                \"query\": description,\n                                \"fields\": [\"description\"]\n                            }\n                        },\n                        {\n                            \"multi_match\": {\n                                \"query\": splade_tokens,\n                                \"fields\": [\"splade\"]\n                            }\n                        }\n                    ]\n                }\n            }\n        }\n    # If SPLADE is not enabled, we only search the description field\n    else:\n        query = {\n            \"query\": {\n                \"multi_match\": {\n                    \"query\": description,\n                    \"fields\": [\"description\"]\n                }\n            }\n        }\n    # Set the number of results to return\n    query['size'] = size\n\n    # Execute the search query\n    response = es.search(index=\"superheroes\", body=query)\n\n    # Extract the hits from the response\n    hits = [hit['_source'] for hit in response['hits']['hits']]\n    return hits\n</code></pre> <p>This function searches for superheroes based on a description (which will be drawn from our alternative  description list). When <code>splade</code> is true, it searches both <code>description</code> and <code>splade</code> fields; otherwise, only the <code>description</code> field.</p> <p>We still need the <code>get_tokens_as_text</code> function to convert descriptions into SPLADE tokens. Note that this doesn't expand the description with synthetic terms, it simply tokenizes it:</p> <pre><code>def get_tokens_as_text(text):\n    tokens = tokenizer(text, return_tensors='pt').input_ids[0]\n    return ' '.join([id2token[i] for i in tokens.tolist()][1:-1])\n</code></pre> <p>Now we're ready to see if this all actually works!</p>"},{"location":"blog/2024/10/09/bridging-the-gap-between-keyword-and-semantic-search-with-splade/#demo-time","title":"Demo Time","text":"<p>Let's take the above code out for a spin.</p> <p>First we index our superheroes with <code>index_superheroes(num_tokens=50)</code>. Here we inject up to 50 SPLADE tokens for each row in our data set.</p> <p>Next, with SPLADE turned off, let's see if we can catch Iron Man using his alternative description:</p> <pre><code>use_splade = false\n\nhero = \"Iron Man\"\nalt_description = hero_dict_alt[hero]\nsearch_results = search_superheroes(alt_description, 3, use_splade)\nresult_heroes = [result['name'] for result in search_results]\n\nprint(result_heroes)\n</code></pre> <pre><code>['Beast']\n</code></pre> <p>Nope... that's a miss! Well, after I've spent all this time writing a blog post, I hope that we can turn SPLADE back on and see Iron Man in the results.</p> <pre><code>['Black Panther', 'Iron Man', 'Beast']\n</code></pre> <p>Yay! I mean, I would have preferred that Iron Man was number 1 in the search results. But being in the top 3 results out of 50 for something as generic as \"A brilliant innovator and corporate magnate\" is not bad.</p> <p>But perhaps we were lucky with this example. Let's create a new function <code>recall_at_3</code> that will run through every hero and and see if SPLADE is actually helping us improve recall.</p> <pre><code>def recall_at_3(splade):\n    counter = 0\n    for hero in hero_dict.keys():\n        alt_description = hero_dict_alt[hero]\n        search_results = search_superheroes(alt_description, 3, splade)\n        result_heroes = [result['name'] for result in search_results]\n        # Check if the hero is in the top 3 search results\n        if hero in result_heroes:\n            counter += 1\n\n    # Calculate and return the recall@3 score\n    return counter / len(hero_dict.keys())\n</code></pre> <p>First we test without SPLADE <code>recall_at_3(False)</code> and see that the recall is 28% \u2013 as expected, not great. Now with SPLADE <code>recall_at_3(True)</code> returns (... drum roll please ...) 52%.</p> <p>Alright! (Whew!) So by injecting synthetic tokens into our indexed documents we have improved recall (recall@3 to be precise) by a hefty 24%!</p>"},{"location":"blog/2024/10/09/bridging-the-gap-between-keyword-and-semantic-search-with-splade/#retrospective","title":"Retrospective","text":"<p>I can feel my inner Hindsight Lad jumping up and down in my head. It's time to take a closer, more critical look at what we just accomplished. SPLADE is definitely neat, but it doesn't fix all of the problems we've identified with semantic search.</p> Image borrowed from the some guy on X <p>We've improved recall, but in a longer blog post (which I shall never write) we would also look at how precision changes. The problem is that sometimes the synthetic tokens produced in <code>get_splade_embedding</code> can be... wonky. Take a look at this example:</p> <pre><code>get_splade_embedding(\"mary had a little lamb, it's fleece was white as snow\", 15)\n</code></pre> <pre><code>['marriage',\n 'married',\n 'winter',\n 'song',\n 'wedding',\n 'have',\n 'sheep',\n 'whites',\n 'baby',\n 'like',\n 'color',\n 'wearing',\n 'film',\n 'character',\n 'murder']\n</code></pre> <p>There's a lot going on here. We start off with several words related to marriage (which is not mentioned in the original song) and then right at the end it takes a darker turn with <code>murder</code>. You know how the rest of that song goes, and these words are clearly a miss. There are also a couple of stop words (super common words) in there: <code>have</code>, and <code>like</code>. This will definitely increase recall as it will match about half of the docs in the index, but this will take it's toll on precision.</p> <p>Next, my SPLADE implementation in Elasticsearch is oversimplified. If you scroll back up to <code>get_splade_embedding</code>, we extract non-zero elements from <code>vec_np</code> (the SPLADE tokens) but discard their associated weights. This is a missed opportunity. The SPLADE papers use these weights for scoring matches. Incorporating this nuance \u2013 for instance, the fact that murder is less relevant to Mary than sheep, song, baby, and white \u2013 would significantly enhance precision.</p> <p>Finally, one of the problems with semantic search that we were trying to avoid is the complexity of dealing with the embedding model when it doesn't quite do what you want it to do. When an embedding model doesn't match the correct documents, then your only option is to retrain the model, reindex, and hope. But with SPLADE, if it thinks that Mary likes murder, our options aren't much better. The main benefit of SPLADE in this case is that you can actually see the words produced by the model, (rather than an opaque vector). This will make it easier to debug the problem and improve it. ... Maybe SPLADE's training data had too many references to Mary I of England (you know... \"Bloody Mary\").</p>"},{"location":"blog/2024/10/09/bridging-the-gap-between-keyword-and-semantic-search-with-splade/#conclusion","title":"Conclusion","text":"<p>SPLADE is a promising approach that bridges the gap between traditional keyword search and modern semantic search. And this is a good thing! In many ways, good ol' keyword search is the right tool because it's relatively simple, it's well understood, and it's easy to scale and maintain. But traditional keyword search still falls short when it comes to matching on meaning.</p> <p>This post is begging for follow-up posts:</p> <ul> <li>How does my implementation of SPLADE+Elasticsearch affect precision?</li> <li>How does semantic search perform against my implementation of SPLADE+Elasticsearch?</li> <li>Can we improve SPLADE+Elasticsearch? I want to see how tough it is to get the SPLADE weights into the Elasticsearch scoring.</li> <li>Did you know that Elasticsearch offers a SPLADE-like solution called ESLER? I wonder how that compares with the solution presented here.</li> </ul> <p>If you're interested in hearing more about this topic, then let me know. We could write a post together about it.</p>"},{"location":"blog/2024/10/09/bridging-the-gap-between-keyword-and-semantic-search-with-splade/#before-you-go-exciting-news","title":"Before You Go: Exciting News!","text":"<p>While we're on the topic of innovative technologies, I'm thrilled to announce that I'm authoring a book on LLM Application Development, set to release in November 2024. This book distills years of experience building production-grade LLM applications at GitHub and for various consulting clients into practical, actionable insights.</p> <p>What's in it for you?</p> <ul> <li>Insider tips and tricks from real-world LLM projects</li> <li>Strategies to overcome common challenges in LLM application development</li> <li>A comprehensive guide to building robust, scalable LLM solutions</li> </ul> <p>Are you currently working on an LLM application and facing roadblocks? Or perhaps you're looking to leverage LLMs in your next big project? I'd be delighted to lend my expertise. Reach out to me at jfberryman\ufe6bgmail\u2027com for consulting inquiries or just to chat about the exciting world of LLMs!</p> <p>Let's push the boundaries of what's possible with LLMs together!</p>"},{"location":"blog/2026/02/05/anthropic-skills--prime-example-of-red-riding-hood-principle/","title":"Anthropic SKILLs \u2013 Prime Example of Red Riding Hood Principle","text":"<p>In Albert and my book, Albert introduced the \"Red Riding Hood Principle\". You remember the story, right? A young, naive girl strays off of the well trodden path and ends up in a lot of trouble.</p> <p>This is true for you when building AI applications. If you provide context to the agent that is familiar \u2013 similar to the training \u2013 then the agent will be able to navigate the terrain more easily.</p> <p>Anthropic SKILLs is such a good example of this. Anthropic realized that in Claude Code, it had trained a model and constructed an agent to be exceptionally good at navigating file systems, reading files, and managing context. Further, the filesystem metaphor provides natural navigational affordances. The agent can look at the directory structure and get a big picture of what exists, and an agent can grep around for details \u2013 much like a developer would do.</p> <p>You should consider all of this when building your own agents! SKILLs benefits from the filesystem metaphor, so it bears to reason that your domain could benefit as well \u2013 imagine presenting graph-based knowledge or filter-based search as if it was a file structure.</p>"},{"location":"blog/2026/02/05/anthropic-skills--prime-example-of-red-riding-hood-principle/#hey-and-if-you-liked-this-post-then-maybe-we-should-be-friends","title":"Hey, and if you liked this post, then maybe we should be friends!","text":"<ul> <li>I just wrote a book about Prompt Engineering for LLM Applications, which has now been published in 6 languages! Maybe you'd be interested in reading it.</li> <li>Are you stumped on a problem with your own LLM application? Let me hear about it.</li> <li>I'm going to write lots more posts. Subscribe and you'll be the first to know.</li> </ul>"},{"location":"blog/2025/10/25/context-engineering-requires-ai-empathy/","title":"Context Engineering Requires AI Empathy","text":"<p>A big part of context engineering comes down to empathy. ... Does this sound surprising?</p> <p>Consider this, LLMs have been trained to act like humans. So when you are building an AI agent, it's a useful exercise to put yourself in their shoes and walk around a bit. For me, I like to think of the AI agent as if it's an AI intern showing up for its first day of work. How would you feel if you were coming in for your first day of work and the boss gave you 50 pages to read? What if you only learned what you were supposed to do with this information after you had already read the 50 pages? And what if the instructions were poorly written, ambiguous, and impossible to achieve with the tools provided!?</p> <p>In this post I'll go over several places where I've learned to empathize with the AI Intern. But understanding the world from their unique vantage point, you can build better context for the agents and drastically improve the quality of your AI application.</p> <p></p>"},{"location":"blog/2025/10/25/context-engineering-requires-ai-empathy/#bright-but-not-psychic","title":"Bright but not psychic","text":"<p>The AI intern is exceptionally bright - he has read the internet several times and has memorized much of it. However, the intern is not psychic \u2013 he can't know information that is hidden behind corporate walls (client information, documentation). Therefore you have to provide this information for him or give him tools to get at it (context engineering). The intern also needs tools to access realtime data.</p> <p>Similarly, he has no knowledge of current events \u2013 a search tool comes in handy here, and the frontier models are getting really good at knowing when they need to use search.</p>"},{"location":"blog/2025/10/25/context-engineering-requires-ai-empathy/#prefers-familiarity","title":"Prefers familiarity","text":"<p>In training, the intern has read lots of markdown, lots of html, plenty of standardized forms; the intern is familiar with commonly used processes and niche jargon. Therefore, if you can frame the intern's work to make use of familiar ideas and patterns, then you won't have to spend so much time explaining the task, and the intern is much more likely to just get it right.</p> <p>In our book (see below) we called this the Red Riding Hood principle \u2013 stay on the well trodden path because wolves are hiding in the forest.</p>"},{"location":"blog/2025/10/25/context-engineering-requires-ai-empathy/#cant-read-gobbledygook","title":"Can't read gobbledygook","text":"<p>This is where prompt engineering is important. If you dump tons of information and scraps of text into the context without explaining it, then you're like the bad boss that slams the thick manilla folder on the intern's desk and yells \"deal with it!\" Instead, take time to help the intern understand their task:</p> <ul> <li>In the system message, lay out the context for the work. What are we doing? What is the goal we are trying to accomplish? What is a good outcome?</li> <li>Tell the intern their role in all of this. How are they to act? What tone should they take?</li> <li>Give the intern instructions on how to accomplish their goal. If he has clear steps to follow, the work will be easier to accomplish.</li> <li>Provide examples. The AI intern has amazing pattern recognition and a few examples do wonders \u2013 again, this is especially true if there aren't many examples in training.</li> <li>When providing the intern with context (documents, images), tell him how these might be of use in solving their problem. If you just have scraps of text, give the intern some sense of the context that this text was found within.</li> <li>Provide the intern with tools to do their work. Make sure that all tools are distinct \u2013 avoid confusing overlaps in domain. Use clear naming conventions and rich descriptions for the tool and argument names. If you wouldn't be able to understand how to use these tools, then your intern is sunk!</li> </ul>"},{"location":"blog/2025/10/25/context-engineering-requires-ai-empathy/#needs-time-to-think","title":"Needs time to think","text":"<p>Your AI intern, just like you, does better when they have time to think. Before extracting complicated information from documentation, ask yourself if you would need to think a bit before performing the task. If so, give your intern the same allowance.</p> <p>The AI intern thinks by speaking (like I do... which is sometimes annoying) \u2013 let them do it! Challenge them to reason about the answer prior to providing the final result, or use a reasoning model which has been trained to be good at talking to itself about the problem. Once the intern has jabbered on for a bit, they will more likely to arrive at a more well-reasoned answer.</p>"},{"location":"blog/2025/10/25/context-engineering-requires-ai-empathy/#has-difficulties-with-attention","title":"Has difficulties with attention","text":"<p>The intern is paradoxically both inattentive and hyperfocused.</p> <p>Inattention: If you drop long chunks of text into the prompt \u2013 especially if they have long chunks of unrelated content \u2013 then at some point in the middle, the intern gets bored, quits paying attention, and misses important details. In our book, we refer to this place as the \"Valley of Meh\".</p> <p>Hyperfocus: But, if the AI intern feels like he's really on a roll with his current approach, then he will sometimes follow that approach too far and fail to recognize that it's no longer beneficial.</p> <p>The fix for inattention requires strong context engineering. The middle chapters of our book focuses on this: 1) select content carefully,  2) assign scores and priorities to content based upon how relevant it is,  3) extract the core pieces of content that are most relevant, and 4) assemble into a prompt document that clearly lays out the significance of the text and how it is to be used.</p> <p>The remediation for hyperfocus is more about good application design, (which is covered in the latter chapters of the book.) Don't give your intern important and complex tasks without also giving them oversight. This could come in the form of a workflow that breaks complex tasks into simpler steps, or it could come in the form of a user experience that keeps the user in the loop, guiding and correcting the intern when they make small, forgivable mistakes.</p>"},{"location":"blog/2025/10/25/context-engineering-requires-ai-empathy/#is-a-convincing-liar-and-sycophant","title":"Is a convincing liar and sycophant","text":"<p>Ah... the darker side of our AI intern. In order to keep you impressed, and to keep his job, you will often catch your intern in a lie or in unnecessary flattery. To deal with this:</p> <ul> <li>Never give the intern a mission critical task that can't either be verified algorithmically or by a human that has oversight.</li> <li>The most common form of oversight is fact grounding - making sure that the individual claims are justified by content in the provided context. You can ground the claims yourself by creating a side-agent that picks out claims and verifies them individually.</li> <li>If your agent is acting as a classifier, but you don't know all the possible classes, then make sure the agent has the option to say \"other\" or \"unknown\", otherwise you are compelling him to lie.</li> <li>Make sure that your agent has the tools that he needs to get the job done. If you ask an agent for information about a recent event, but he doesn't have a web search tool, then he is likely to just make up a story to appease you. If he has the search tool, then it will more likely go to the internet and get the facts he needs.</li> </ul>"},{"location":"blog/2025/10/25/context-engineering-requires-ai-empathy/#has-unusual-vision-and-reading-disabilities","title":"Has unusual vision and reading disabilities","text":"<p>The AI intern has some pretty unusual vision and reading disabilities. For one thing, he can't really see letters. He sees tokens, which are like blurry chunks of letters. This sometimes causes confusion, like when you ask the intern for how many 'R's are in 'strawberry' and he says 2.</p> <p>The intern also isn't so great at reading tabular information without a lot of support. And it makes sense that it would be difficult, because, whereas I can look at a spreadsheet and pull in the 2 dimensional structure at a glance, your intern sees one row at a time serialized as a long stream of text. This means that he has to do a lot of work to keep up with which column corresponds to which field and how everything is related.</p> <p>In both of these cases, the situation is improving: - Models are getting more powerful, and even though they can't see the letters in the tokens, they understand that they are there. Models are getting better at keeping track of tabular content too. - Models are increasingly getting better vision capabilities that do allow them see at a glance how content is structured. - There are also plenty of services and libraries emerging that act as prosthetics, allowing models to make better sense of tabular data and give them the ability to query and navigate such data.</p>"},{"location":"blog/2025/10/25/context-engineering-requires-ai-empathy/#has-anterograde-amnesia","title":"Has anterograde amnesia","text":"<p>The intern \u2013 poor fella \u2013 has a case of anterograde amnesia. He can't make new memories. When your intern shows up to work for day 2, or day 20, or day 200, it's just like day 1 to them. Unlike a human, they can't make mistakes, learn from them, and then recall them in the future. So - you must provide this for them.</p> <p>For starters, everyday when the intern sits down to start work (that is, for every single request), make sure you have him read the new employee onboarding documentation - at least the important parts (context engineering!), because otherwise, they don't really know how work is done at this company.</p> <p>Next, consider building a prosthesis for their memory. As the intern works with your users and makes mistakes, record them. And then, next time they appear in a similar situation, provide that information back to the intern so they will be able to make a better decision.</p>"},{"location":"blog/2025/10/25/context-engineering-requires-ai-empathy/#prompt-engineering-for-llms","title":"Prompt Engineering for LLMs","text":"<p>If you've found this post interesting, then you should read my book, Prompt Engineering for LLMs. Don't be fooled by the title, large portions of the book cover context engineering in great detail. It's just that while developing our approach at GitHub Copilot, the term \"context engineering\" didn't yet exist. The book also covers lots of information about how LLMs work (your intern's psychology and neurology), and it covers the philosophy of LLM application design.</p> <p>And take a moment to follow me everywhere: - This blog - Twitter - LinkedIn</p>"},{"location":"blog/2024/11/11/cut-the-chit-chat-with-artifacts/","title":"Cut the Chit-Chat with Artifacts","text":"<p>Most chat applications are leaving something important on the table when it comes to user experience. Users are not satisfied with just chit-chatting with an AI assistant. Users want to work on something with the help of the assistant. This is where the prevailing conversational experience falls short.</p> <p></p> <p>Consider pair programming. In a real, human pairing session, you and your partner discuss your objectives, talk about how the code should be modified, and then take turns actually modifying the code to implement your ideas. In this scenario \u2013 and in most where work is to be done \u2013 there is the discussion and then there are the objects of the discussion.</p> <p>Contrast this with the naive AI assistant chat in which the assistant is not able to make the distinction between the discussion and the things being discussed. The assistant may come up with fantastic ideas about how to write your report or accomplish your task, but those ideas are quickly lost in the scrollback. And if there are multiple objects floating around in the discussion, then it's nearly impossible to tell the assistant which objects you're talking about and which version and how they relate to one another. At the end of the conversation, the user might find themselves scrolling back to copy out pieces of the conversation that they need.</p> <p>The answer to this problem is artifacts. Artifacts, are referenceable chunks of stateful content. Artifacts are the objects of the discussion and the items being worked upon. Both the assistant and the user have the ability to create, retrieve, update, and delete these artifacts and to refer to them as needed.</p> Simple Illustration of a Conversation with Artifacts <p>In this post I will show you how to step beyond the status quo and build a better user experience with artifact-aware AI assistants.</p> <p>Blog posts not your thing?</p> <p>Here's an 8 minute video that covers the juicier points.</p> <p> </p>"},{"location":"blog/2024/11/11/cut-the-chit-chat-with-artifacts/#status-quo-no-artifacts","title":"Status Quo \u2013 No Artifacts","text":"<p>But first, let's take a closer look at the status quo experience, just to drive home the pain. In the demo below, the user (blue text) is a real estate agent. The real estate agent is working with an AI assistant to prepare a home listing email to send to a client. (Note: the empty panel on the right is intended to hold artifacts \u2013 we'll put it to good use in a moment.)</p> <p>There's a lot going on here.</p> <ul> <li>First, the real estate agent asks the assistant to retrieve a home listing. The assistant complies \u2013 and also proactively retrieves the email template \u2013 but, unless the agent digs through the tool usage, they don't know anything more about the listing than what the assistant tells them. Is this even the right listing?</li> <li>Next, the real estate agent asks the assistant to prepare the email. The assistant complies by generating a draft in their follow-up message. The annoying part here is that there is no boundary between the assistant's text and the object that we are working on. It's just one big blob of text.</li> <li>The buyer's name and agent's names have been omitted, so the agent asks the assistant to update the email with the correct names, but also to add their name to the template. The assistant complies with the email, but ignores the request to fix the template because it doesn't know how to comply.</li> <li>If the real estate agent wants to use the email, they have to scroll around and copy-paste it out and send it themselves. This is unnecessary toil.</li> </ul> <p>This is not be a good experience. The user feels lost (not able to see the original data), confused (they can't see the data that the assistant can see), and overburdened (it's on you to extract the work and apply it). And if the conversation were to continue, it would only get worse. More items will be discussed, many of them will have several versions, and all of it will be scattered in the scrollback and effectively lost.</p>"},{"location":"blog/2024/11/11/cut-the-chit-chat-with-artifacts/#change-is-in-the-air-as-companies-move-toward-artifacts","title":"Change is in the Air as Companies Move Toward Artifacts","text":"Anthropic Artifacts: AI-generated diagrams and documents OpenAI Canvas: Interactive content creation workspace Cursor: Project-aware AI coding assistant Hex: AI-powered data analytics and dashboards <p>Some major players are beginning to explore the potential of artifact-based interactions. Anthropic\u2019s Artifacts and OpenAI\u2019s Canvas allow users to describe diagrams, documents, and simple applications, which then materialize alongside the conversation. While these tools offer a glimpse into new UX possibilities, they still feel like prototypes focused more on form than function. For instance, Anthropic\u2019s Artifacts lack direct editing capabilities, making even small adjustments cumbersome, and limiting their utility for serious work.</p> <p>In contrast, companies like Cursor and Hex are using artifacts to drive tangible productivity. Cursor provides software developers with a project-aware assistant that listens to requests, suggests file changes, and lets users apply edits selectively. By clearly separating the conversation from project files, Cursor gives both the user and assistant a better mental model of the task, leading to a more productive workflow. Compare this to copy-pasting swaths of code back-and-forth into ChatGPT. (I did a lot of this before Cursor!)</p> <p>Similarly, Hex empowers data scientists by combining notebooks, SQL, Python, and AI to create interactive dashboards and analytics. Its \"magic\" AI assistant enhances workflow by tracking both the conversation and the artifacts (e.g. dashboards and datasets). In conversation, users can \"@-reference\" artifacts, and instruct the assistant to generate new dashboards. In this way it's easy for analysts (and even non-analysts) to quickly piece together dashboards for their company.</p>"},{"location":"blog/2024/11/11/cut-the-chit-chat-with-artifacts/#the-new-state-of-the-art-artifacts","title":"The New State of the Art \u2013 Artifacts","text":"<p>Let's take another look at our real estate application, but this time let's make it artifact aware.</p> <ul> <li>The real estate agent starts by asking the assistant to retrieve the home listing, and it does. But this time the assistant response includes a link to one of the artifacts in the artifact panel on the right. There the agent can see the full details of the home listing. In this simple demo the artifacts are just bare JSON. But in a real application, the home listing would include a image carousel, property details, interactive maps, and integrated scheduling for viewings.</li> <li>The agent asks the assistant to create the email according to their saved template. This time two new artifacts appear. The first is the template retrieved from the <code>get_email_template</code> tool call. The second is customized email generated by the assistant which pulls the contents of the listing into the template.</li> <li>Finally, the agent tells the assistant to correct the names in the email and to update the template. And the assistant does as it's told! In the conversation it provides links to the updated artifacts and explains the actions taken.</li> </ul> <p>This experience is so much better than before. It's intuitive because it's how conversation work in real life: you have a conversation about your work. In the left panel is the conversation and in the right panel are the work items of work being referred to. When the assistant talks about the work, it conveniently links to the actual artifact so that you can review the full details. The assistant understands that it can create, retrieve, and update artifacts \u2013 which leads to a much more coherent interaction. And you don't have to copy-paste assets out of the scrollback. If this were a real application, you would likely even send the email directly from the app!</p> <p>Now You Try!</p> <p>If you have a moment, try the demo yourself and compare the difference between the two assistants:</p> <ul> <li>Naive assistant</li> <li>Artifact-aware assistant</li> </ul> <p>Note that I've added some suggested comments to get you started. Make sure to let me know if you find anything interesting (... or broken).</p>"},{"location":"blog/2024/11/11/cut-the-chit-chat-with-artifacts/#implementing-artifact-aware-assistants","title":"Implementing Artifact-Aware Assistants","text":"<p>Artifact-aware assistants require coordinated implementation in the backend, frontend, and system message. Fortunately it's actually rather simple.</p>"},{"location":"blog/2024/11/11/cut-the-chit-chat-with-artifacts/#system-message","title":"System Message","text":"<p>In order to build an artifact-aware assistant, the first thing you need to do is to convey to the model what artifacts are, and how they work. Here's the system message that I used to build the above demo.</p> Artifacts System Message <pre><code>You are a helpful assistant.\n\n&lt;artifacts_info&gt;\nArtifacts are self-contained pieces of content that can be referenced in the conversation. The assistant can generate artifacts during the course of the conversation upon request of the user. Artifacts have the following format:\n\n\u02cb\u02cb\u02cb\n&lt;artifact identifier=\"acG9fb4a\" type=\"mime_type\" title=\"title\"&gt;\n...actual content of the artifact...\n&lt;/artifact&gt;\n\u02cb\u02cb\u02cb\n\n&lt;artifact_instructions&gt;\n\n- The user has access to the artifacts. They will be visible in a window on their screen called the \"Artifact Viewer\". Therefore, the assistant should only provide the highest level summary of the artifact content in the conversation because the user will have access to the artifact and can read it.\n- The assistant should reference artifacts by `identifier` using an anchor tag like this: `&lt;a href=\"#18bacG4a\"&gt;linked text&lt;/a&gt;`.\n- If the user says \"Pull up this or that resource\", then the assistant can say \"I found this resource: &lt;a href=\"#18bacG4a\"&gt;linked text&lt;/a&gt;\".\n- The linked text should make sense in the context of the conversation. The assistant must supply the linked text. The artifact title is often a good choice.\n- The user can similarly refer to the artifacts via an anchor. But they can also just say \"the thing we were discussing earlier\".\n- The assistant can create artifacts on behalf of the user, but only if the user asks for it.\n- The assistant will specify the information below:\n    - identifiers: Must be unique 8 character hex strings. Examples: 18bacG4a, 3baf9f83, 98acb34d\n    - types: MIME types. Examples: text/markdown, text/plain, application/json, image/svg+xml\n    - titles: Must be short, descriptive, and unique. Examples: \"Simple Python factorial script\", \"Blue circle SVG\", \"Metrics dashboard React component\"\n    - content: The actual content of the artifact and must conform to the artifact's type and correspond to the title.\n- To create an artifact, the assistant should simply write the content in the format specified above. The content will not be visible to the user in chat, but instead will be visible in the Artifact Viewer. After creating an artifact, they can refer to it in the conversation using an anchor tag as described above. Example:\n    \u02cb\u02cb\u02cb\n    HUMAN: Create a simple Python int sort function.\n    ASSISTANT: I will create a simple Python merge sort function.\n    &lt;artifact identifier=\"18bacG4a\" type=\"text/markdown\" title=\"Simple Python int sort function\"&gt;\n    def sort_ints(ints):\n        if len(ints) &lt;= 1:\n            return ints\n\n        mid = len(ints) // 2\n        left = sort_ints(ints[:mid])\n        right = sort_ints(ints[mid:])\n\n        # Merge sorted halves\n        result = []\n        i = j = 0\n        while i &lt; len(left) and j &lt; len(right):\n            if left[i] &lt;= right[j]:\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n\n        result.extend(left[i:])\n        result.extend(right[j:])\n        return result\n    &lt;/artifact&gt;\n\n    It is available in the Artifact Viewer as &lt;a href=\"#18bacG4a\"&gt;Simple Python int sort function&lt;/a&gt;.\n    \u02cb\u02cb\u02cb\n- The assistant can edit artifacts. They do this by simply rewriting the artifact content.\n- If the user asks the assistant to edit the content of an artifact, the assistant should rewrite the full artifact (e.g. keeping the same identifier, but modifying the content and the title if needed).\n- The user doesn't have to explicitly ask to edit an \"artifact\". They can just say \"modify that\" or \"change that\" or something similar.\n- When editing the artifact, you must completely reproduce the full artifact block, including the identifier, type, and title. Example:\n    \u02cb\u02cb\u02cb\n    HUMAN: Make that sorting function sort in descending order.\n    ASSISTANT: &lt;artifact identifier=\"18bacG4a\" type=\"text/markdown\" title=\"Simple Python int sort function (descending)\"&gt;\n    def sort_ints(ints):\n        if len(ints) &lt;= 1:\n            return ints\n\n        mid = len(ints) // 2\n        left = sort_ints(ints[:mid])\n        right = sort_ints(ints[mid:])\n\n        # Merge sorted halves in descending order\n        result = []\n        i = j = 0\n        while i &lt; len(left) and j &lt; len(right):\n            if left[i] &gt;= right[j]:  # Changed &lt;= to &gt;= for descending order\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n\n        result.extend(left[i:])\n        result.extend(right[j:])\n        return result\n    &lt;/artifact&gt;\n    \u02cb\u02cb\u02cb\n- All existing artifacts are presented in the &lt;artifacts&gt; tag below.\n&lt;/artifact_instructions&gt;\n\n&lt;/artifacts_info&gt;\n\n&lt;artifacts&gt;\n&lt;artifact identifier=\"ab3f42ca\" type=\"application/json\" title=\"123 Maple Street Listing\"&gt;\n{\n    \"address\": \"123 Maple Street\",\n    \"price\": 450000,\n    \"bedrooms\": 3,\n    \"bathrooms\": 2,\n    \"sqft\": 1800,\n    \"description\": \"Charming craftsman with original hardwood floors\",\n    \"yearBuilt\": 1925,\n    \"status\": \"For Sale\"\n}\n&lt;/artifact&gt;\n&lt;/artifacts&gt;\n</code></pre> <p>The approach here is straightforward.</p> <ul> <li> <p>Explain what an artifact is - a formatted blob of information that takes the form</p> <pre><code>&lt;artifact identifier=\"d3adb33f\" type=\"application/json\" title=\"The Title\"&gt;\n    ... content ...\n&lt;/artifact&gt;\n</code></pre> <p>Further, explain the expected format and constraints of the fields.</p> </li> <li> <p>Explain that the user can see these the artifacts, and therefore the assistant does not need to recreate them in its messages. Instead, the assistant should refer to the artifact using a link formatted as <code>&lt;a href=\"d3adb33f\"&gt;link text&lt;/a&gt;</code>.</p> </li> <li>Explain that the assistant can both create and modify artifacts by retyping them. I've included a couple of example interactions to help the model out.</li> <li>Finally, present the existing artifacts to the assistant.</li> </ul> <p>This simple system message works quite well even though the model I'm using, claude-3-5-sonnet, is not trained on artifacts. I think the reason for this is because the models are used to text that includes references. In natural language we use, nicknames and pronouns. In programming we refer to variables and packages. And in HTML \u2013 which is found in abundance in training \u2013 we use links! Thus, the model has ample training to differentiate the content of a conversation from the objects of discourse.</p>"},{"location":"blog/2024/11/11/cut-the-chit-chat-with-artifacts/#backend-implementation","title":"Backend Implementation","text":"<p>A naive assistant (not aware of artifacts) is implemented as a loop which keeps track of messages in a conversation. When a user submits a message, the assistant:</p> <ol> <li>Appends the user message to the existing messages.</li> <li>Sends the message list to the model and then retrieves the response message. (If you use tool calling, then that also happens here.)</li> <li>Appends the response message to the list of existing messages as the assistant message.</li> <li>Sends the assistant message back to the user.</li> </ol> <p>With an artifact-aware assistant, you have to keep track of both the messages and the artifacts, so there are a couple of extra steps. When a user submits a message, the assistant:</p> <ol> <li>Extracts any artifacts from the user message (for instance if the user created a new work item) and replaces them with links.</li> <li>Generates the system message, which contains both the instructions and the list of all artifacts.</li> <li>Append the user message to the existing messages.</li> <li>Sends the system message and conversation messages to the model. In the demo we are using tool calling, so there's also a for-loop handling tool invocations.</li> <li>Extract artifacts that are either generated by the assistant or retrieved from a tool call and replace them with links.</li> <li>Send the assistant message and the artifacts back to the user.</li> </ol> <p>I must make a note on artifact extraction. In my implementation, the assistant is able to modify existing artifacts. It does this by simply rewriting the artifact in the conversation. (In the demo, this is automatically replaced with links, so the user only ever sees the artifacts in the artifact panel.) Since the assistant can rewrite artifacts, this means that there can be multiple versions of the same artifacts. The way I'm handling this is to use the most recent version and delete the old version of the artifact. In a more sophisticated implementation, perhaps we would track the changes to the artifacts so that the assistant can understand their history.</p>"},{"location":"blog/2024/11/11/cut-the-chit-chat-with-artifacts/#frontend-implementation","title":"Frontend Implementation","text":"<p>The frontend requires changes as well. Most notably, you need a place to present the artifacts. In the demo, this is a dedicated panel to the right of the conversation. If your UI doesn't have room for that, then there are alternatives. For instance, you can have a tabbed chat window that allows the user to flip over and see the artifacts. Or you can still incorporate the artifacts into the chat as embedded UI elements. This loses some of the benefit because your artifacts will scroll away as the conversation continues, but at least the artifacts aren't just blobs of text - they can be made into \"smart\" objects that the user can interact with.</p> <p>The chat panel requires a small update. The backend will now return messages that include links to the tabs in the artifacts. Make sure that these links look nice and, most importantly, reveal the corresponding item in the artifact panel when clicked.</p> <p>Next, unlike with Anthropic Artifacts, why not let the user directly edit the artifacts? Make sure to capture the edits and send the updated artifacts to the backend.</p> <p>Finally, I haven't done this with my demo, but if the assistant is creating and updating artifacts, it is probably important to make sure the user can understand the changes and explicitly accept or reject them. Perhaps the best approach is to follow Cursor's lead and present the users with a GitHub-style red/green diff of the changes, and a button beside each that allows the user to accept or reject the change.</p> <p>Check It Out</p> <p>If you'd like to see how the sausage is made, check out the repo that implements the demo here:</p> <p>https://github.com/arcturus-labs/artifact-aware-assistant</p> <p>Warning, it is not production-ready code!    </p>"},{"location":"blog/2024/11/11/cut-the-chit-chat-with-artifacts/#conclusion","title":"Conclusion","text":"<p>If you are curious about Artifact assistance, fortunately they're not hard to set up! The demo that I prepared for this blog post is just the tip of the iceberg. Consider other possibilities that artifacts unlock:</p> <ul> <li>Interactive artifacts that presented themselves in a malleable user interface. For instance a home listing, complete with an image carousel and an interactive map.</li> <li>Durable artifacts that save themselves to disk as they are created and updated. Such as a modified email template.</li> <li>Active artifacts that accomplish real work. Imagine an email artifact that sends itself with the click of a button.</li> <li>Rich versioning, allowing the user to traverse the changes associated with this artifact and link to the portion of the conversation where the change occurred.</li> </ul> <p>I bet that you can think of plenty more things!</p> <p>Special thanks to Doug Turnbull, Freddie Vargus, Bryan Bischof, and Julia Neagu for providing feedback on this post.</p>"},{"location":"blog/2024/11/11/cut-the-chit-chat-with-artifacts/#hey-and-if-you-liked-this-post-then-maybe-we-should-be-friends","title":"Hey, and if you liked this post, then maybe we should be friends!","text":"<ul> <li>I just wrote a book about Prompt Engineering for LLM Applications. Maybe you'd be interested in reading it.</li> <li>Are you stumped on a problem with your own LLM application? Let me hear about it.</li> <li>I'm going to write lots more posts. Subscribe and you'll be the first to know.</li> </ul>"},{"location":"blog/2025/01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/","title":"Fire Yourself First: The E-Myth Approach to Iteratively AI App Development","text":"<p>I've always been interested in entrepreneurship, so, early on in my career, I asked my financial advisor for book recommendations about startups. He handed me \"The E-Myth\" by Michael Gerber \u2013 a book about... building food service franchises? In the heat of the dot-com explosion, this wasn't exactly the startup guide I was hoping for, but its core message stuck with me and turned out to be surprisingly relevant to the problems I hear about regularly when talking to people about building reliable LLM applications.</p> <p></p> <p>The E-Myth's central insight is really quite simple: in any business, there are tasks to be done, and each task has steps. As a founder, you start by doing everything yourself, but that won't scale. The solution? Document and systematize those steps so that you can train others to do the work \u2013 effectively \"firing yourself\" from each task.</p> <p>This approach actually serves as a great metaphor for how you should iteratively build and improve LLM applications! Let's dive in.</p> <p>Blog posts not your thing?</p> <p>Text got you down? Then check out the completely unedited companion YouTube video. Watch me deliver the same information as the post, and in the same time that reading would have required.</p> <p> </p>"},{"location":"blog/2025/01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/#the-problem-with-typical-llm-development","title":"The Problem with Typical LLM Development","text":"<p>As Simon Willison astutely observes: \"The key skill in getting the most out of LLMs, is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\"</p> <p>He's right. LLMs are remarkably capable, but they're also gullible and prone to hallucination which makes it is easy for them to get off track when reasoning through complicated tasks. This makes building complex LLM applications on the first try basically impossible. Yet many teams attempt to do exactly that \u2013 diving straight into building hordes of complex AI agents without first understanding the human processes they're trying to automate.</p> <p>The solution? Take a page from the E-Myth: Start by doing the work manually, document your processes, and then systematically automate each task. Don't try to build an AI agent that can do everything \u2013 build a system where you can gradually fire yourself from specific, well-defined tasks and let the LLM take over.</p>"},{"location":"blog/2025/01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/#a-real-world-example-the-recruiting-startup","title":"A Real-World Example: The Recruiting Startup","text":"<p>Let me share a recent example that serves as a great illustration for this approach. I recently spoke with a founder who was single-handedly building a startup that focused on recruiting. His platform needed to:</p> <ol> <li>Analyze r\u00e9sum\u00e9s and extract structured data</li> <li>Evaluate candidate quality</li> <li>Match candidates with job postings</li> <li>Handle communication with candidates throughout their job search</li> </ol> <p>Instead of attempting to build an all-encompassing AI solution from day one, he started with a process that was largely manual. He personally handled each step, which allowed him to understand the nuances and challenges of each task.</p> <p>Then, piece by piece, he began to automate. First, he \"fired himself\" from data extraction \u2013 a repetitive task with clear inputs and outputs. Next came candidate quality evaluation. Since this was already somewhat subjective, transitioning to AI-generated ratings (after careful validation) made sense \u2013 he built an LLM-as-judge setup that would read resumes and rate candidates on a five point scale.</p> <p>His next target? The communication workflow. But rather than haphazardly building an agent to do all communication with candidates, he's taking an incremental approach by initially just automating email drafts.</p> <p>From here, there are plenty of things that he can work on next, at each step letting the work itself dictate which task is most applicable for handing over to an LLM. Perhaps he'll find that the communication drafts require too much human editing, and he may introduce a more involved assistant workflow to pen those communications (for instance starting with an email outline that will be easier for the recruiter to review and update). Another possible next step is to introduce evaluation and improvements in the candidate ranking process to ensure that the LLM-as-judge generally agrees with human judges.</p>"},{"location":"blog/2025/01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/#incrementally-incorporating-agency-into-your-application","title":"Incrementally Incorporating Agency into your Application","text":"<p>Here are some general guidelines to help you incrementally incorporate more and more AI Agency into your application.</p>"},{"location":"blog/2025/01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/#1-identify-your-first-target","title":"1. Identify your First Target","text":"<p>Create a list of tasks that might be good candidates for replacement. We're most likely looking for one of two forms:</p> <ol> <li>Tasks that are easily described as a set of steps with well-defined inputs and outputs. </li> <li>Tasks that involve a human working together with an LLM assistant in order to accomplish a goal. </li> </ol> <p>For the current state-of-the-art for LLMs, it's advisable to stay away from complex, open-ended reasoning tasks \u2013 remember, these models are incredibly capable, but not incredibly reliable just yet.</p> <p>Look for tasks that are expensive and timeconsuming for a human to complete, such as extracting structured data from documents or generating content. Also, look for tasks that are more amenable to occasional failure, because getting 100% correctness from a non-deterministic component such as an LLM is going to be really challenging.</p>"},{"location":"blog/2025/01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/#2-map-the-human-process","title":"2. Map the Human Process","text":"<p>For each module:</p> <ul> <li>Identify the inputs and outputs, and make sure they are well-defined.</li> <li>Document the steps that a human would take in accomplishing the task of the model. This needs to be something you could explain to a new employee, because you're about to onboard the AI agent as your new employee. Does the agent need tools? Be able to explain them. Will the agent be assisting a human? Be able to explain the expectations of the interaction.</li> <li>Track which steps are repetitive or time-consuming, but not overly complex. This will give you an idea about where to get started when replacing yourself with an AI agent.</li> </ul>"},{"location":"blog/2025/01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/#3-fire-yourself-make-the-robots-do-the-work","title":"3. Fire yourself! Make the Robots do the Work","text":"<p>If your application is well-modularized, then ideally you can replace chunks of code with their LLM-enhanced equivalents. The idea is to keep the well-defined inputs and outputs, and keep the goals the same, but replace the human interaction part with an AI-assisted version. Naturally, this is sometimes this is easier said than done.</p> <p>Nobody likes eating their veggies it seems, but it's also important to perform evaluations on your new modules to make sure they have the desired behavior. However, here too, you can be incremental in your approach. If the module is amenable to occasional failure, and if there is plenty of other lower-hanging fruit that you need to tackle, then your evaluations can be pretty superficial at first. Try 20 different inputs of varying forms, throw some curveballs and corner cases at the module, and make sure that none of the outputs would be damaging to your application or embarrassing to your company. Eventually though, you should certainly come back and perform proper evaluations (more on that in a future post.)</p>"},{"location":"blog/2025/01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/#4-iterate-recurse","title":"4. Iterate! Recurse!","text":"<p>Create a pattern of continual optimization. As soon as you fire yourself from one position and hire an AI agent, check your list for the next task that can be turned over to the agent. Keep doing this until you start running into diminishing returns. Fortunately, you're racing against a technology that is improving by leaps and bound every day. With any luck, that wall of diminishing returns will continue to be pushed in front of you.</p>"},{"location":"blog/2025/01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/#conclusion","title":"Conclusion","text":"<p>The path to reliable agentic applications isn't through ambitious, all-at-once automation. It's through systematic, incremental improvement. Start by doing the work yourself, understand it deeply, and then gradually fire yourself from each task by building focused, reliable AI agents.</p> <p>This approach might seem slower at first, but it's far more likely to succeed. You'll build something that actually works, understand where the limitations are, and know exactly how to evaluate and improve each component.</p>"},{"location":"blog/2025/01/17/fire-yourself-first-the-e-myth-approach-to-iteratively-ai-app-development/#hey-and-if-you-liked-this-post-then-maybe-we-should-be-friends","title":"Hey, and if you liked this post, then maybe we should be friends!","text":"<ul> <li>I just wrote a book about Prompt Engineering for LLM Applications. I bet you'd be interested in reading it.</li> <li>Is your company working on AI Agents? I'd love to hear about it. I can help you stay ahead of the curve and make the most of AI technology.</li> <li>I'm going to write lots more posts like this. Subscribe, and know as soon as new content lands..</li> </ul>"},{"location":"blog/2026/01/18/incremental-ai-adoption-for-e-commerce/","title":"Incremental AI Adoption for E-commerce","text":"<p>When you think of e-commerce, your mind is probably drawn to Amazon.com as \"the definitive\" example. But it's actually the exception. The internet is filled with tons of small- and medium-sized e-commerce sites. These sites typically follow the same pattern - a search page with a search box at the top, selectable filters along the left side, and results filling the remainder of the screen. And the whole goal is to quickly usher customers to the products they seek.</p> <p>For most of these sites, the implementation is quite simple. Product metadata is indexed into a search engine such as Elasticsearch or Algolia. This includes fields like the title of the product, its description, its price, and other relevant features (sizes for shoes, square feet for houses, etc.) And the application is typically quite simple \u2013 the user submits a search, and the backend issues a query that hopefully captures the customers intent, and then captures the responses and sends them to the frontend for display in the search results.</p> <p>Unfortunately \"right-out-of-the-box\" search results are often not that great, and fixing the problem often requires hiring a team of search experts \u2013 something that smaller shops are unable to afford. Fortunately, modern AI is coming to the rescue! In this post we'll demonstrate how e-commerce shops can incrementally adopt AI and explore improvements in search which would have been unbelievable just 5 years ago.</p> <p></p> <p>Want to see it in action?</p> <p>Not in the mood for reading? Watch the video version where I walk through the entire progression with live demos:</p> <p> </p> <p>Or jump straight to the live demo to try it yourself: CLICK HERE</p>"},{"location":"blog/2026/01/18/incremental-ai-adoption-for-e-commerce/#dispelling-the-magic-of-rag-and-agentic-ai","title":"Dispelling the Magic of RAG and Agentic AI","text":"<p>In 2024 \"RAG\" \u2013 Retrieval-Augmented Generation \u2013 became a very popular application of AI. It was sold to us as magic \u2013 just plug in RAG and point it at a set of documents and then you would instantly have a world-class agentic search application \u2013 like... \"poof\"! But the problem was that RAG was a black box component, and if something went wrong, then it was nigh impossible to identify the problem and fix it.</p> <p>In another post I make black-box RAG transparent.  I explain how it's really a set of pipelines \u2013 one for indexing content, and another for retrieving it \u2013 For the sake of this post you just need to know that RAG isn't anything special; it's just a call to an LLM which has access to a well-described product search tool that is backed by any form of search including \"old school\" lexical search.</p> <p>2025 was \"the year of Agentic AI\". This was also amazingly overblown. No one could agree even on the definition of \"agentic\" at first, and when all the dust settled by the end of the year, we found out that Agentic AI was really, literally, just a couple of for-loops. The inner for-loop wrapped the LLM call and allowed \"the agent\" to make multiple sequential tool calls, learn from the responses, and continue until it had the results it needed. The outer for-loop incorporated the user input so that the agent actually acted like an AI Assistant, responding to the user's course corrections.</p> <p>But the big point for this post is that nothing is magical. A request to Gen AI is just an HTML request \u2013 text in, text out. Build an agent by wrapping that with a couple of for-loops. And make the agent a RAG agent by giving it your current search endpoint as a tool. Easy-peasy!</p>"},{"location":"blog/2026/01/18/incremental-ai-adoption-for-e-commerce/#morphing-traditional-search-into-modern-ai-search","title":"Morphing Traditional Search into Modern AI Search","text":"<p>Now let's see what this looks like from your customer's perspective. I'll walk through four levels of AI adoption, each building on the previous.</p>"},{"location":"blog/2026/01/18/incremental-ai-adoption-for-e-commerce/#level-0-traditional-search-your-current-reality","title":"Level 0: Traditional Search (Your Current Reality)","text":"<p>Most e-commerce sites today look like this:</p> <p></p> <p>You've got a keyword search box at the top, a sidebar full of filters, and the remaining space for the search results. The burden is entirely on the user to understand the typical terminology used in the documents, learn what filters exist and how they work, and sort the results correctly.</p> <p>The reality? Most users type a query, scan the first page, and leave if nothing looks right; \"Their search doesn't understand me.\"  Users don't explore filters and often type text in the search box which really should be a selected filter. For instance, in a home search, they might say \"4BR\" or \"Under $750K\" instead of selecting the right filters. </p> <p>Unless you've hired that top-notch machine learning team, queries like these aren't parlayed into the relevant results. You're probably seeing lots of confused usage patterns and correspondingly high bounce rates. Unfortunately, this is just par for the course for most traditional e-commerce applications. </p>"},{"location":"blog/2026/01/18/incremental-ai-adoption-for-e-commerce/#level-1-beginner-ai-test-the-waters","title":"Level 1: Beginner AI (Test the Waters)","text":"<p>Let's take a baby step. In this step, we basically keep traditional search unchanged, but add only a small suggestion bar that appears after results load.</p> <p></p> <p>The AI interprets the user's natural language query and suggests a search that takes better advantage of filters and sorting, for example \"Did you mean this? Property Type: Condo, Search Terms: 'Downtown'\". Users can click to apply these suggestions or ignore them completely.</p> <p>Implementing this is quite easy \u2013 simply define a search tool that has well-specified arguments associated with the keyword search text, facet selections, and sorting. Then, whenever a user issues a search, you asynchronously send the request to this simple \"agent\" and instruct it to make the best search it can based upon the users' input. One great thing about the agent, is that it will automatically recover from misspellings and odd terminology. You can improve upon this further when writing the prompt: include examples and explanations so that the agent has an understanding of the vocabulary used in the products and the options available for the facets.</p> <p>Notice that this baby step is effectively risk-free for the user experience \u2013 the search app works no differently from what they are used to in the traditional app above. There is no extra latency incurred because the AI request is asynchronous. And the only concession we've made is about 50 pixels of vertical real estate on the screen.</p> <p>At this point you should be gathering data about your users' experiences. Track every time that the user is exposed to the \"did you mean\" text, track how often they click the associated search button, and track how often they convert. If the click-through and conversion are an improvement over the traditional search, then your users are resonating! Time for the next step.</p>"},{"location":"blog/2026/01/18/incremental-ai-adoption-for-e-commerce/#level-2-intermediate-ai-let-ai-take-the-wheel","title":"Level 2: Intermediate AI (Let AI Take the Wheel)","text":"<p>The easy and obvious next step is to let the AI actually execute the recommended search rather than waiting for their approval. See how we've replaced the \"did you mean\" text with \"interpreted as\" in the image below.</p> <p></p> <p>At this point you are incurring some real UX risk, because the snappy 10ms response your customers have become used to is now being replaced by a response that could take 2 or 3 seconds. As an old-school search engineer myself, this makes the sweat bead on my brow because \"you must never let latency increase\". But, if it takes your user 5 seconds to come up with a good query, and the AI can come up with that good query in two seconds, then we are actually saving the user some effort. What's more, the user's first query is never good, so they will either query multiple times to get what they want, or they will succumb to cognitive overload and abandon search. AI is saving them a lot of effort.</p> <p>Notice one more change in the intermediate AI experience. We have added a new box just above the results which contains a summary of the results and some recommended next queries. The summary gives the user a holistic understanding of the search results, saving them from having to look through the results if they aren't quite what was expected; and the recommended next queries keep the user engaged with search, so that rather than abandoning, they might get new ideas for related content.</p> <p>As always, it's important to measure everything in order to understand what is working and what isn't. A/B test the beginner and intermediate experiences. Does automatic query rewriting lead to better or worse conversion? Are users engaging with the recommended next queries? If so... on to the advanced AI experience.</p>"},{"location":"blog/2026/01/18/incremental-ai-adoption-for-e-commerce/#level-3-advanced-ai-full-conversational-assistant","title":"Level 3: Advanced AI (Full Conversational Assistant)","text":"<p>Notice, during the intermediate AI phase, that in generating a summary of the results and recommended future queries, the AI is effectively having a conversation with the user. The only problem is that it's stateless, and it's one-sided \u2013 Issue a new query and the agent forgets all about the previous one. Does the user want to redirect the agent? They're out of luck.</p> <p></p> <p>With the advanced AI experience, we morph the traditional search experience into a guided, conversational experience. In this example, we ditch the search box, and replace it with a chat window on the right side of the screen.</p> <p>I'm a huge proponent of conversational UX. Why? Because it was the first UX \u2013 humans have been having conversations for 50 thousand years! It's only been in the last 30 or so years when clicking glowing rectangles has become the default UI/UX \u2013 and in my opinion, we only did this to address a technical limitation \u2013 machines didn't talk until just recently!</p> <p>When user's start interacting conversationally with your e-commerce app, you'll witness a Cambrian explosion of new user behaviors. For one thing, they will start leaning into the intuition of the model. Rather than taking time to type in \"3-4 bedroom house over 2500sq ft\" maybe they'll just say \"Show me some big ass houses!\" (why not?). The agent (now, really the \"assistant\") will attempt to map the user's wishes into a query.</p> <p>In traditional search, we only have the text from the keyword search box, and identifying user intent often requires your search application to be psychic (hint... it's not). But now since the conversation is stateful, magic happens. The user works with the assistant to clarify their intent, and the resulting queries are much better targeted, leading to much higher conversion rates.</p> <p>The assistant has also gained some other superpowers. Rather than just pasting results on the screen, the assistant can talk about the results. The user can ask \"which one of these are more modern style?\" The user can ask for simple aggregate analyses over the results: \"what is the typical price distribution for houses of this size?\". Leaning further into the assistants common-sense understanding the inventory, the user's intent, and the world at large, the user can brainstorm with the agent about future searches. Finally \u2013 maybe this is level 4 \u2013 you can make the agent perform asynchronous research on behalf of the user and really dig through the inventory once it feels like it has a good understanding of the user's needs.</p> <p>Regarding product measurement: traditional metrics still matter (conversion rates, A/B testing), but since conversations can drift into many domains, use LLMs to analyze collected conversations and map out typical user journeys. You're going to learn a lot more about your customers than you ever did just talking with them through the keyword search box!</p>"},{"location":"blog/2026/01/18/incremental-ai-adoption-for-e-commerce/#easier-than-you-think","title":"Easier Than You Think","text":"<p>The entire demo you just saw \u2013 including the whole, fake traditional search and fake inventory \u2013 took me 10 hours to build (ain't vibe-coding amazing?!). If you are interested in incrementally adopting AI, then nothing much has to change. Your existing search infrastructure (Elasticsearch, etc.) stays in place. All you have to do is write a thin, completely decoupled endpoint that looks at the user request, and attempts to map that into a proper search request (again... using your current system), and you incorporate that into the UI. If that works out to your expectations, then just make the integration thicker, bit-by-bit.</p> <p>Are the results really effective? Yes! Just take my app for a spin yourself and see. CLICK HERE. After the page loads (first load might takes a few seconds) make sure to play with the demo controls at the top which let you try out each phase of AI integration. Start with traditional search, load some of the example queries, and then notice how the queries are interpreted, then move on to the more advanced AI integrations.</p> <p>Or, for e-commerce businesses, if you really want the \"easy\" button to speed you down your AI roadmap, then let's chat. I can help you navigate from traditional search to conversational AI experiences with minimal the risk or complexity.</p> <p>The future of e-commerce search is conversational, and getting there is simpler than ever.</p>"},{"location":"blog/2026/01/18/incremental-ai-adoption-for-e-commerce/#hey-and-if-you-liked-this-post-then-maybe-we-should-be-friends","title":"Hey, and if you liked this post, then maybe we should be friends!","text":"<ul> <li>I just wrote a book about Prompt Engineering for LLM Applications, which has now been published in 6 languages! Maybe you'd be interested in reading it.</li> <li>Are you stumped on a problem with your own LLM application? Let me hear about it.</li> <li>I'm going to write lots more posts. Subscribe and you'll be the first to know.</li> </ul>"},{"location":"blog/2025/12/07/programming-in-english/","title":"Programming in English using Cursor Commands","text":"<p>I had a fun little experiment over the weekend. I needed to understand a codebase quickly, so I started interrogating it using Cursor. Around the same time, I discovered cursor commands and realized: \"I need this capability often, let's formalize it.\" But it morphed into something bigger.</p> <p></p> <p>Rather watch than read?</p> <p>Here's a quick video where I walk through everything in this post.</p> <p> </p>"},{"location":"blog/2025/12/07/programming-in-english/#what-i-built","title":"What I Built","text":"<p>I wanted a general hierarchical way to understand different questions about codebases. Not just \"what does this file do\" but also potehtially other things: - \"Tell me about this repo\" - \"How does data flow through the system\" - \"Explain how the codebase is modularize \u2013 and the interfaces are between the modules\" - Anything relating to the codebase</p> <p>I wanted something hierarchical: a quick overview first, easy to click through to see the code, and easy to drill down for further details.</p>"},{"location":"blog/2025/12/07/programming-in-english/#how-it-works-the-explain-in-issue-command","title":"How It Works: The Explain-in-Issue Command","text":"<p>The command follows a few simple steps:</p> <p>Step 0: Understand the request. Verify context, research if needed.</p> <p>Step 1: Create hierarchical markdown structure. The command generates: - A main README with an overview - Separate detail files for each section - Extensive code links to GitHub (with line numbers) - Inline links to the details files linked as <code>([details](./file.md))</code> - Inline links to online references linked as <code>([ref](url))</code></p> <p>Step 2: Create GitHub issue. The README becomes the issue description, and each detail file becomes a comment. The command captures all the comment URLs.</p> <p>Step 3: Link everything together. Replace file links with GitHub comment URLs and update the issue description with the final links.</p> <p>The output is a single link to the explanation issue which is really easy to share with others and really easy to navigate by just clicking around on the links. See the video above for a demonstration of this in action. You can also see the actual output, an example issue explaining a toy chatbot repo that I built.</p>"},{"location":"blog/2025/12/07/programming-in-english/#the-big-idea-programming-in-english","title":"The Big Idea: Programming in English","text":"<p>Here's what's really cool about this: I created a natural language markdown file that executes as a program. Heck, the \"program\" itself was vibe-coded\u2014meta-programming in natural language. This is literally programming in English.</p> <p>The command is just a specification document that gets executed step-by-step. This represents a new paradigm: natural language specifications as executable workflows.</p> <p>It's super flexible\u2014it can even do research outside of software. Check out this Japanese-US economic relations example where it researched the web and provided justified references with links. As best I can tell, Cursor is quickly becoming the \"anything agent\" \u2014 and I wonder if they have even noticed this at this point (... we should talk Aman, Sualeh).</p>"},{"location":"blog/2025/12/07/programming-in-english/#conclusion","title":"Conclusion","text":"<p>Simple idea, powerful results. If you want to try it yourself, here's the cursor command gist. Stick it in your Cursor today \u2014 I think it'll be useful to you, too.</p> <p>What I'm strarting to see is even more than natural language as spec \u2014 this is the English spec being run as code, and the large language model agent being its runtime. It's a brave new world, folks!</p>"},{"location":"blog/2025/12/07/programming-in-english/#building-something-cool-with-ai","title":"Building something cool with AI?","text":"<p>If you're experimenting with programming in English, vibe-coding your way to automation, or building your own \"anything agent,\" I'd love to hear about it. Drop me a line \u2014 I'm always curious what others are discovering in this space.</p> <p>And if you want to stay in the loop on more posts like this, subscribe here and you'll be the first to know when new content lands.</p>"},{"location":"blog/2025/06/17/recipes--a-pattern-for-common-code-transformations/","title":"Recipes \u2013 A Pattern for Common Code Transformations","text":"<p>I did a thing. A very silly, very meta thing. I vibe-coded a CLI tool that summarizes YouTube videos, recorded myself making the tool, and then used the tool to summarize the video of me making the tool. And now, dear reader, you are reading a blog post that was largely generated from that summary. </p> <p>But the real star of the show isn't the tool, or the video, it's the Recipe Pattern \u2013 a way to encapsulate repetitive coding work into a one-off, reusable doc.</p> <p></p>"},{"location":"blog/2025/06/17/recipes--a-pattern-for-common-code-transformations/#whats-a-recipe","title":"What's a \"Recipe\"?","text":"<p>Let's say you've written a CLI tool you're proud of. You like the way it's structured, the coding pattern it uses, and the way it installs. Now you want to reuse the same patterns again, but for a different project. Do you:</p> <ul> <li>A) Spend 2 hours manually refactoring and then forget what you were doing?</li> <li>B) Have a long conversation with Cursor to instruct it how to create the new CLI tool?</li> <li>C) Write a Recipe \u2013 a doc that instructs Cursor (or whatever vibe-coding platform you're using) exactly how to repeat the magic, step by step?</li> </ul> <p>Sorry, the answer is D - none of the above. But C is close. The only thing is that rather than you writing a recipe, you have Cursor itself extract the recipe from a codebase.</p> <p>A recipe is a markdown file that describes how to perform a coding task that you only perform occasionally. In this case I'm making a recipe that takes a rough piece of code and converts it to a well-formed and easy-to-install CLI tool.</p> <p>Recipes are perfect for:</p> <ul> <li>Converting code snippets into CLI tools (the example task of this post)</li> <li>Bootstrapping new project structures (think LLM-powered cookiecutter)</li> <li>Auto-generating dashboards from your ORM</li> <li>Creating a migration script for converting from any framework/pattern/language to any other</li> <li>Reading a codebase and creating a standardized README</li> <li>Rewriting one (small) codebase from one language into another one</li> <li>Converting a run script into a minimal Dockerfile that bakes in your preferences</li> <li>Automating any somewhat complex coding task you'll need to do occasionally, but you don't want to memorize</li> </ul>"},{"location":"blog/2025/06/17/recipes--a-pattern-for-common-code-transformations/#whats-the-difference-between-recipes-and-cursor-rules","title":"What's the Difference Between Recipes and Cursor Rules","text":"<p>In one sense... nothing. A recipe is a blob of text that is used to inform Cursor (or whatever vibe coding tool your using) about how to proceed \u2013 which sounds like a rule. However the utility of Cursor rules is a bit different than what I'm getting at with recipes. Cursor rules are often applied in all chats \u2013 they influence the general behavior of the agent. Recipes on the other hand are useful when you need them, but they are rarely used \u2013 so you definitely don't want to apply them to most Cursor sessions, and you probably don't even want them taking up space in your Cursor rules menu.</p> <p>The other neat thing I'm pointing out about recipes in this blog post (especially if you watch the video below) is that you don't have to write them yourself, you can point Cursor at a codebase and tell it to extract a recipe based upon the pattern it sees in the codebase. And then later, when you need it, you drop the recipe into the prompt as basically a runbook for transforming the code in this codebase to a form similar to that of the original codebase.</p>"},{"location":"blog/2025/06/17/recipes--a-pattern-for-common-code-transformations/#the-video-extracting-and-applying-the-cli-recipe","title":"The Video: Extracting and Applying the CLI Recipe","text":"<p>Now... back to that video. If you watch it, it shows the process of extracting a reusable CLI recipe from an existing codebase that had a well-structured pattern for CLI tools. I then applied this recipe to another codebase that originally just had a Jupyter notebook for summarizing YouTube videos, turning it into a proper CLI tool. The cool part is how straightforward it was to extract the recipe, and how it can now be used to quickly convert future codebase or code snippets into fully functional CLI tools.</p> <p>So without further adieu, here is the video. As usual - no cuts, no edits. See me in all my glory.</p> <ul> <li>Here's the repo with the final tool implementation.</li> <li>Here's the recipe that created the CLI tool, packaging, and install scripts.</li> <li>Here is the summary of the YouTube video generated using the CLI tool created in the YouTube video.</li> </ul>"},{"location":"blog/2025/06/17/recipes--a-pattern-for-common-code-transformations/#whats-next-a-cli-tool-that-writes-blog-posts","title":"What's Next? A CLI Tool That Writes Blog Posts","text":"<p>Now I've got a recipe for converting scraps of code into well-organized CLI tools.</p> <p>If you thought this was the end of the recursion, think again. As an experiment, I wrote the first draft of this post by prompting Cursor with the text of the YouTube summary and some general guidance about what I wanted. So the obvious thing to do next is to copy the Cursor conversation into a notebook, make a half-assed prompt for writing blog posts, and then apply my new recipe to convert the prompt into a CLI tool that can write a blog post from piped in text. Maybe I'll use that tool to write a blog post about how it wrote itself.</p>"},{"location":"blog/2025/06/17/recipes--a-pattern-for-common-code-transformations/#hey-and-if-you-liked-this-post-then-maybe-we-should-be-friends","title":"Hey, and if you liked this post, then maybe we should be friends!","text":"<ul> <li>I just wrote a book about Prompt Engineering for LLM Applications. Maybe you'd be interested in reading it.</li> <li>Are you stumped on a problem with your own LLM application? Let me hear about it.</li> <li>I'm going to write lots more posts. Subscribe and you'll be the first to know.</li> </ul>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/","title":"Why Spec-Driven Development Breaks at Scale (And How to Fix It)","text":"<p>When GitHub Copilot launched in 2021, AI code completion took the development world by storm. But after a mere year or two, code completion was completely eclipsed by vibe-coding, allowing much larger tasks to be accomplished with much less effort. Vibe-coding is great, but it has some problems that limit its utility. Agents tend to work with the code as if they are over-ambitious interns; they often do more damage than good if you're not guiding them at every step.</p> <p>The most recent trend is spec-driven development. This term is still ill-defined, but the basic idea is that prior to tackling a meaningful code change, you first create a specification document for that change and then use the specification as a guide for the AI to make changes. This helps the agent to better understand the big picture. Once the implementation is complete, you throw away the spec because it has served its purpose.</p> <p>This form of spec-driven dev is a good idea! But I want more! In this post I'll talk about a bigger notion of spec-driven development. I'm talking about an ideal world where we keep track of the global product specification, and then we allow the agent to build code based upon that.</p> <p></p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#the-challenge-of-global-product-specification","title":"The Challenge of Global Product Specification","text":"<p>Imagine that you set out to perform a truly extreme form of spec-driven development: You write your full product specification and then deploy an agent to execute upon the plan. It's going to build your whole website from scratch and return to you the finished product. But there's a problem. 2 days later when the agent finally returns you realize that the generated product is not quite what you wanted. It's not a problem with the agent, let's assume that the agent's work is perfectly consistent with the specification doc \u2013 it's just that your specification was ambiguous. That's the problem with these big specs \u2013 they are written in natural language and natural language is imprecise and ambiguous.</p> <p>So what's the solution? One thing you could do is to specify away the uncertainty! Find everything that is ambiguous in the specification, and add subsections (and sub-subsections) to clarify exactly what you mean. But what you'll find is that in order to make natural language sufficiently precise, you will have to write so much content that you lose any benefit of writing the spec in the first place. The spec has become a formalized language and you might as well use a different formalized language for this task \u2013 code.</p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#how-humans-succeed-where-ai-fails","title":"How Humans Succeed Where AI Fails","text":"<p>In contrast, as humans, we do pretty well with natural language. How is it possible that we do such a reasonable job with large-scale software development when AIs fall flat?</p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#shared-context-and-world-understanding","title":"Shared Context and World Understanding","text":"<p>For starters, we have a shared understanding of the world that exists right around us. An AI, on the other hand, has read every bit of text and code in the public domain \u2013 so it has a great idea of how things generally work. But it has no idea how things typically work at your company and in this codebase, and the context that we provide these agents can't possibly capture the nuance for understanding this.</p> <p>You, on the other hand, have accrued an understanding of your immediate environment through trial and error. When you started working at your company, you implemented your first code change, and in the PR review you learned a LOT about \"the way we do things here\". And you learned more with every interaction you've made at the company both with the code and with the humans in meetings and through hallway conversations. What you've learned can not easily be put into a giant document and shared with the agent as context.</p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#the-power-of-clarification","title":"The Power of Clarification","text":"<p>Next, humans seek clarification. When statements are made, we identify ambiguities, and we engage in conversations to disambiguate the possibilities. What's more, we're quite efficient at only dealing with the ambiguities that really matter. We lean back upon the shared context (last paragraph) and we don't ask about the things that \"everyone is supposed to know.\" We don't ask about which libraries to use, and we don't ask about tiny implementation details. We do ask about things that confuse us or things that are unclear. Models are only now starting to get good at this, and even so, to become really good at clarification, AI agents need the nuanced contextual understanding I present in the last paragraph \u2013 which doesn't exist for them.</p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#building-better-ai-development-workflows","title":"Building Better AI Development Workflows","text":"<p>So how do we address the challenge of specification ambiguity in large-scale spec-driven development? Let's hit the above points in reverse order and talk about both clarification and world view.</p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#enabling-clarification-through-conversation","title":"Enabling Clarification Through Conversation","text":"<p>Regarding clarification \u2013 there is no way that the agent can get to a better understanding if you allow them to ask about the ambiguous or ill-defined portions of the spec. That is, I think if spec-driven dev is to work, it necessarily requires some back-and-forth to identify and nail-down all the ambiguities. This is probably implemented as some sort of chat experience. Another thing you can do for smaller chunks of work is to have the AI implement the spec multiple times so that the different implementations reveal points of ambiguity. The agent can then compare the different implementations and use what it learns to converse with the developer about which direction it should go.</p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#building-agent-world-view-through-hierarchical-specifications","title":"Building Agent World View Through Hierarchical Specifications","text":"<p>The goal of clarification is ultimately to correct and flesh out the agent's world view. They need to know how things typically work at this company and in this codebase. There are a couple of options here. First, recall the idea above about adding subsections and sub-subsections to the spec? This would require a great deal of effort for a developer, but this might be time well spent for an AI agent. I don't recommend literally adding all these subsections to the spec because it would be huge. Instead, there is probably some hierarchical approach that can be used to allow so that the global spec can link out to sub-specification documents. (As a matter of fact, in an earlier post I showed that AI Agents are pretty good at navigating links like this - so it's a natural fit.)</p> <p>It's not clear what the best organization of the hierarchy is though \u2013 you could put one spec in every file, and then in every directory have a rolled up version of that directory's contents \u2013 all the way to the top. In this case, the specification would be rigidly attached to aspects of the code, so a better approach might be something more like a free-form wiki \u2013 but that can get out of hand for other reasons.</p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#code-as-the-most-granular-form-of-specification","title":"Code as the Most Granular form of Specification","text":"<p>At some point, the best way to encode the low-level assumptions of the code is to just use the code itself, because, while there is ambiguity in natural language, there is no ambiguity in the code. This does imply a philosophical change though. Some people are in the camp that a well-defined spec should lead to functionally equivalent generations every time. I disagree with this, basically because I believe it to be infeasible \u2013 natural language just isn't up to the challenge. Relying upon the code itself as the leaf-level spec removes the problem of specification ambiguity because code changes are grounded in the code that already exists. And you never expect to globally regenerate the code from the master product spec.</p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#living-specifications-usage-and-evolution","title":"Living Specifications: Usage and Evolution","text":"<p>The above discussion addresses how the specification comes into existence. The next interesting question is how the spec is used and changed over time. If done correctly, the specification will be of great use for both the humans and the AI agents that are working in the codebase.</p> <p>If the AI agents are aware of the spec and understand how to navigate it, then they will be grounded not only by the code - but the product specification itself. This is actually quite an improvement over the way we humans used to do things: as developers we rarely have access to the product specification, and we often end up trammeling over product decision that have been lost to the past. This is the reason that aging codebases look like inconsistent patchwork quilts \u2013 we did the best we could with the very small context we had at hand \u2013 most of which was extracted by reading code rather than talking with product managers or reading documentation.</p> <p>This is precisely where global, hierarchical specifications will be of their best service. No, developers should be expected to actually read those specifications, but it should be easy for them to have a conversation with the AI agent and suss out the information that is relevant to our work at hand. As the developer scopes out a code change with the agent, the agent will navigate the global spec and call out any places where the implementation is inconsistent with the spec.</p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#specification-evolution-through-code-changes","title":"Specification Evolution Through Code Changes","text":"<p>The last thing to cover is how specifications change with time. Like I said in the intro, we tend to think about specification-driven development like so: you write the spec, break it up into steps, have an AI help implement all the steps, and then you throw away the spec and submit your PR. I think we got it backwards \u2013 I think changes in code should lead to changes in the specification itself. Whenever code changes, it needs to be compared with the existing product specification. If this introduces anything that is inconsistent to the written specification, then the global specification will be edited and submitted in the same PR as the code change.</p> <p>This leads to a richer working experience for everyone on the team. For the engineer, it becomes easier to understand how their code changes are affecting the global specification. The product manager can also become more involved in pull requests because they can read the actual text of the specification change and jump into the conversation if they see anything surprising.</p> <p>A global spec will also give product managers and even executives more insight into the evolution of the product. In the olden days \u2013 say, 5 years ago \u2013 few people actually read documentation because it was difficult to find and rarely up-to-date. In the world we're entering, the global specification will be kept up-to-date automatically. The product manager and the executive still won't read the docs, but they will talk to their AI assistant and get answers about the current state of the product and about how it has evolved over time.</p>"},{"location":"blog/2025/10/17/why-spec-driven-development-breaks-at-scale-and-how-to-fix-it/#the-future-of-spec-driven-development","title":"The Future of Spec-Driven Development","text":"<p>The path forward for large-scale spec-driven development isn't about perfecting natural language specifications \u2014 it's about building systems that can navigate ambiguity through conversation and context. By combining hierarchical specifications with conversational clarification, and by grounding agents in existing code, we can create AI development workflows that are both powerful and consistent.</p> <p>The real breakthrough will come when specifications become living documents that evolve with our codebases, maintained by the same AI agents that implement them. This creates a feedback loop where product decisions are preserved, context is maintained, and the gap between specification and implementation disappears.  </p>"},{"location":"blog/2025/03/31/supercharging-llm-classifications-with-logprobs/","title":"Supercharging LLM Classifications with Logprobs","text":"<p>I was just reading the classification chapter of Jay Alammar and Maarten Grootendorst's excellent book Hands-On Large Language Models. I felt inspired to extend their work and show yet another cool trick you can do with LLM-based text classification. In their work they demonstrated how an LLM can be used as a \"hard classifier\" to determine the sentiment of movie reviews. By \"hard\" I mean that it gives a concrete answer, \"positive\" or \"negative\". However, we can do one better! Using \"this one simple trick\"\u2122 we can make a \"soft\" classifier that returns the probabilities of each class rather than a concrete single choice. This makes it possible to tune the classifier \u2013 you can set a threshold in the probabilities so that classifications are optimally aligned with a training set.</p> <p></p> <p>Videos more your cup of tea?</p> <p>I'm a slow reader - so I often go to YouTube to have ideas beamed directly into my brain. And now you can do the same thing! Watch me summarize this post in just 10 minutes! Outtakes never taken out.</p> <p> </p>"},{"location":"blog/2025/03/31/supercharging-llm-classifications-with-logprobs/#building-the-hard-llm-classifier","title":"Building the \"Hard\" LLM Classifier","text":"<p>To quickly repeat/paraphrase the work of Hands-On Large Language Models, here's how to build a sentiment classifier using an LLM.</p> <ol> <li>Create a prompt that explains the task: \"Find the sentiment of this movie review.\"</li> <li>Add in the text to be classified.</li> <li>Explain the output format for the classification: \"Please say only 'positive' or 'negative' and no other words or explanations.\"</li> </ol> <p>Here's how we could couch this inside of a function:</p> <pre><code>def get_sentiment(text, model):\n    messages = [{   \n        \"role\": \"user\",\n        \"content\": f\"\"\"Read the following text and tell me if the sentiment is positive or negative: \n\n        &gt; {text}\n\n        Just say 'positive' or 'negative' (lowercase - no other text - no quotes no words besides positive or negative)\"\"\",\n    }]\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        max_tokens=1,\n        temperature=0.7,\n    )\n\n    return response.choices[0].message.content\n</code></pre> <p>Let's use it a couple of times:</p> <pre><code>&gt;&gt;&gt; get_sentiment(\"this sucks\", \"gpt-4o-mini\") \nnegative\n\n&gt;&gt;&gt; get_sentiment(\"it's awesome\", \"gpt-4o-mini\")\npositive\n</code></pre> <p>Perfect.</p> <p>Let's try one more time:</p> <pre><code>&gt;&gt;&gt; get_sentiment(\"this sucks, it's awesome\", \"gpt-4o-mini\")\npositive\n</code></pre> <p>Positive? Really? I mean, surely there's some nuance to that, right? Let's try it again a few more times \u2013 click, click, click \u2013 and we get another 2 positives and a negative. So there is nuance. But how can we understand what the nuance is? How can we take advantage of this?</p>"},{"location":"blog/2025/03/31/supercharging-llm-classifications-with-logprobs/#turning-a-hard-classifier-into-a-soft-classifier","title":"Turning a \"Hard\" Classifier into a \"Soft\" Classifier","text":"<p>The solution is obvious, right? If you run the classifier several times, then the ratio would eventually converge to the true value, and we would have our soft classifier. But the problem with this solution is just as obvious \u2013 how many times do you have to run the classifier before it converges on the correct solution? The answer... lots. And I ain't got that kind of time or money.</p> <p>But would you believe that you can get the exact probabilities in a single LLM completion request? You can, but first you need to read cool info box about logprobs.</p> <p>Cool Info Box About Logprobs (or \"The Lies We Tell Ourselves\")</p> <p>When an LLM makes a completion, it doesn't just magically come back with the text all at once. That's a lie. Instead, it looks at the prompt and generates a single token, and then this token gets appended to the prompt and the calculation happens all over again. And one token at a time, the completion is calculated.</p> <p>But this is also a lie. Because the LLM doesn't really come up with the next token. It's actually a 2-step process. First the LLM looks at the prompt, and rather than predicting the next token it actually comes up with a long list of probabilities for every possible next token. For instance, gpt-4o has roughly 100K next tokens \u2013 if you prompt it with \"Today's weather is\" then every possible next token is associated with a probability. Among them will be \"sunny\", \"cloudy\", \"hot\", \"cold\", \"rainy\", all with reasonably high probabilities. But there will also be all the other tokens \"pickle\", \"manly\", \"dance\", \"even\" most with infinitesimally small probabilities. And if you summed up all the probabilities, they would add up to 1.0 \u2013 that is, there's a 100% chance that one of these tokens will be the next token - nothing less, nothing more.</p> <p>There is one more complexity to cover. (Yes, this was another lie.) Each token is actually associated with a \"logprob\" rather than a probability - that is, the logarithm of the probability rather than the actual probability. Why? It's just easier computationally. It's also no big deal \u2013 you can convert from logprobs to probabilities by taking the exponent of the logprob.</p> <p>Oh... yeah, and all of that was a lie too. If you really want to know what's happening inside the LLM, I recommend you again to Hands-On Large Language Models \u2013 in particular, chapter 3. But my explanation here is sufficient for now.</p> <p>With that knowledge in hand, here is how to extend the hard classifier above to make our soft classifier. First you have to follow the exact same steps above to make the hard classifier. But for the request, you ask for the logprobs to be returned. You don't want the probabilities of all ~100K tokens, but 10 or so should be enough. Then, when you get the completion back, you extract the logprob for each of the possible next tokens and convert them into probabilities.</p> <p>Let's take a look at the implementation:</p> <pre><code>def get_sentiment(text, model):\n    messages = [{   \n        \"role\": \"user\",\n        \"content\": f\"\"\"Read the following text and tell me if the sentiment is positive or negative: \n\n        &gt; {text}\n\n        Just say 'positive' or 'negative' (lowercase - no other text - no quotes no words besides positive or negative)\"\"\",\n    }]\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        max_tokens=1,\n        temperature=0.7,\n        logprobs=True, # this instructs the model to return the logprobs for each token returned\n        top_logprobs=10, # this further instructs the model to return the top 10 logprobs, not just the one selected\n    )\n\n    # Extract top logprobs and convert to probabilities\n    logprobs_list = response.choices[0].logprobs.content[0].top_logprobs # we only care about the first token in the completion\n    token_probs = {\n        # item.token is the text of the token - \"positive\", \"negative\", or something else\n        # math.exp(item.logprob) is the probability of that token\n        item.token: math.exp(item.logprob)\n        for item in logprobs_list\n    }\n\n    # for the sentiment classification, I only care about the probability of 'positive' and 'negative'\n    # but I'm lumping the probability for all other possible tokens into \"other\"\n    pos_prob = token_probs.get('positive', 0)\n    neg_prob = token_probs.get('negative', 0)\n    other_prob = 1 - (pos_prob + neg_prob)\n\n    return {'positive': pos_prob, 'negative': neg_prob, 'other': other_prob}\n</code></pre> <p>To see how it works, let's run this sentiment classifier with the problematic example from above:</p> <pre><code>&gt;&gt;&gt; get_sentiment(\"this sucks - it's awesome\", \"gpt-4o-mini\")\n{\n    'positive': 0.5621647747752282,\n    'negative': 0.4378143668101077,\n    'other': 2.085841466414884e-05,\n}\n</code></pre> <p>There's our nuance! Before, we ran the hard classifier 4 times and found that it was positive 75% of the time and negative 25% of the time. But with the hard classifier it would be impossible to have an accurate estimate without running the classification many, many times. But if you have access to the underlying probabilities (and we do!) then you don't have to run the classifier multiple times. You can just look and immediately see what tiny fraction of the time the model would have classified the sentiment as positive vs. negative.</p> <p>What's more, you can set a threshold for the probability of the classifier and tune its performance to maximize your desired criteria. For example, it would be naive to assume that if the value associated with <code>positive</code> is greater than <code>negative</code>, this implies that the sentiment is positive. There might be some bias in the classifier \u2013 but you can tune for it! You can run the soft classifier on a labeled sample of your dataset and adjust the cutoff threshold so that accuracy for your sample is optimized. This might mean that any value for <code>positive</code> that is above, say, 0.47 is counted as being positive.</p>"},{"location":"blog/2025/03/31/supercharging-llm-classifications-with-logprobs/#making-your-own-classifier","title":"Making Your Own Classifier","text":"<p>If you want to make a similar classifier, then here are some ideas and tips to consider:</p> <ul> <li>Sentiment analysis only has 2 options \"positive\" or \"negative\", but your implementation could have many more. For instance, you could make a tech support classifier that classifies emails into the portion of the product that is being discussed.</li> <li>Sentiment analysis requires almost no explanation, but your classifier could incorporate a prompt with considerable explanation and some examples. Maybe you could even employ chain-of-thought or reasoning prior to declaring its final answer.</li> <li>Notice that in the above implementation I was keeping track of the probability associated with \"other\" tokens besides \"positive\" and \"negative\", this is because ambiguous text can make the classifier jump to a token besides \"positive\" and \"negative\", such as \"ambiguous\". A better classifier would take this into account. There are several options here:<ol> <li>You can normalize the probabilities so that prob(\"positive\") and prob(\"negative\") sum to 1.0 and ignore the other tokens.</li> <li>You can listen to what the LLM is trying to tell you \u2013 catalog the other tokens being referred to and incorporate them into new classifier with more options.</li> <li>Add an explicit \"none of the above\" option.</li> </ol> </li> <li>Whenever you implement a \"soft\" classifier make sure that each of the classes you select are single tokens. For instance, \"positive\" and \"negative\" are tokens, but if \"positive\" was two tokens \"pos\" and \"itive\", then this technique wouldn't work - at least not without some extra considerations. One easy way to get around this is to use numbered options for your classes.</li> </ul>"},{"location":"blog/2025/03/31/supercharging-llm-classifications-with-logprobs/#conclusion","title":"Conclusion","text":"<p>There are a lot of fun things you can do with logprobs. For instance, when we were working at GitHub, the coauthor on my book, Albert Ziegler, was experimenting with a way to pack the optimum number of few shot examples into a prompt. He did this by measuring the \"perplexity\" of the text of the Nth example, which is derived from the logprobs of that text.</p> <p>Unfortunately, not to many frontier models these days actually surface logprobs \u2013 to my knowledge, only OpenAI. And I can see why not \u2013 full access to logprobs makes it much easier to distill the knowledge of a teacher model into a student model. So even OpenAI has limited access to only output tokens (which means that, you can't do Albert's trick any more!). But if you're using OpenAI or hosting your own open-weights model then building a classifier like this is still a neat tool to have in your belt.</p> <p>Oh! Shout-out to Jay Alammar and Maarten Grootendorst. If you really want to grok how things work inside the model and learn some neat approaches for using them, then make sure to grab a copy of their book. It's fantastic!</p>"},{"location":"blog/2025/03/31/supercharging-llm-classifications-with-logprobs/#hey-and-if-you-liked-this-post-then-maybe-we-should-be-friends","title":"Hey, and if you liked this post, then maybe we should be friends!","text":"<ul> <li>I just wrote a book about Prompt Engineering for LLM Applications. Maybe you'd be interested in reading it.</li> <li>Are you stumped on a problem with your own LLM application? Let me hear about it.</li> <li>I'm going to write lots more posts. Subscribe and you'll be the first to know.</li> </ul>"},{"location":"blog/2025/03/31/visual-reasoning-is-coming-soon/","title":"Visual Reasoning is Coming Soon","text":"<p>I gotta say \u2013 I love it living in exponential times. I can just wish that something existed and then within a month it does! This time it happened with OpenAI's 4o image generation release. In this blog post I'll briefly cover the release and why I think it's pretty cool. Then I'll dive into a new opportunity that I think is even more exciting \u2013 visual reasoning.</p> <p></p> <p>Rather watch than read?</p> <p>Hey, I get it - sometimes you just want to kick back and watch! Check out this quick video where I walk through everything in this post. Same great content, just easier on the eyes!</p> <p> </p>"},{"location":"blog/2025/03/31/visual-reasoning-is-coming-soon/#why-image-manipulation-with-llms-stinks","title":"Why Image Manipulation with LLMs Stinks","text":"<p>Working with images in Multimodal LLMs has been a mostly one-sided affair. On one hand, it's really cool that you can drop an image into an LLM conversation and get the model to reason about it. But when you ask the model to generate an image, there is a disconnect, because all the model can do is describe the image in text and then call out to an external image generation tool to generate the image based on that text. Text is a poor communication medium for images, and the resulting image is often quite disconnected from the expected result because the short description that the LLM provides to the image generation tool will rarely capture the full context of the conversation.</p> <p>The problem is most pronounced when attempting to go back and forth working on an idea for an image. You can show the LLM an image of your cat and then say \"make this cat wear a detective hat and a monocle\". The best the model can do is to put a detective hat and monocle on some cat, not the one in your image. To make matters worse, the model can't even see the image that it has just created. So if you ask for a modification to first generation attempt, then the subsequent generations are really just starting over from scratch and hoping that a more detailed description to the image generation tool will make things better... it won't.</p> Left: OpenAI's pet cat. Right: The best a traditional LLM can do when asked to give it a detective hat and monocle."},{"location":"blog/2025/03/31/visual-reasoning-is-coming-soon/#openais-release-brings-true-image-manipulation-to-llms","title":"OpenAI's Release Brings True Image Manipulation to LLMs","text":"<p>OpenAI's image generation release changes all of this. OpenAI's newest GPT-4o model carries the full context of the conversation into the image generation, and rather than call out to an external model, it is the very same GPT-4o model that does the image generation. By carrying the full context of the conversation \u2013 including the previous images \u2013 into the generation of the new image you can generate much more consistent imagery.</p> <p>Thus, stealing OpenAI's example, you can show it a photo of your cat, tell it to give your cat a detective hat, et voila! It works!</p> Left: OpenAI's pet cat. Right: GPT-4o's generation when asked to give the same cat a detective hat and monocle. Much more consistent! <p>Now if all we could do with this technology is stick silly costumes on our cats, then this would honestly be a pretty lame achievement. But think about what we're angling for here. You'll be able to turn hand-drawn garbage into sophisticated infographics and marketing material. You'll be able to take crappy matplotlib charts and reformat them as professional-looking plots that go into slides. You'll be able to try on clothes virtually before you buy them. You'll be able to create photo collages from your photos, game assets, and 3D models. You'll be able to edit your vacation photos to remove that one person who blinked. You'll be able to take a photo of your living room and try out different furniture arrangements and color schemes. You'll be able to take a photo of your garden and get suggestions for where to plant new flowers. The possibilities are endless when you can manipulate images with the full context of a conversation.</p>"},{"location":"blog/2025/03/31/visual-reasoning-is-coming-soon/#visual-reasoning-will-be-the-real-innovation","title":"Visual Reasoning will be the Real Innovation","text":"<p>All of the above is but prologue for what will come next. Models perform better when they take time to reason. This is because models have no internal monologue. Whereas humans pause and think about a question prior to answering, models have to generate tokens in order to think. But to this point models have only been able to reason in the space of words, and words are ill-suited for reasoning through many types of real world problems. But soon the models' new image manipulation capabilities will power a new form of reasoning. Right now, this ability is mostly useful for image editing. Soon, models will have the ability to visualize different scenarios and visually reason about the world around them.</p>"},{"location":"blog/2025/03/31/visual-reasoning-is-coming-soon/#demoing-visual-reasoning","title":"Demoing Visual Reasoning","text":"<p>Let's try out an example right now and get a glimpse of how visual reasoning will work. For our experiment we'll use Matthew Berman's marble in a cup problem \u2013 it's one of his go-to reasoning challenges when playing around with a new model:</p> <p>I've put a marble in a glass. The glass is then turned upside down and put on a table. Then the glass is picked up and put into the microwave. Where is the marble?</p> <p>It's a great problem because it requires common sense knowledge of how objects work in the world, and it's especially suited for this post because it requires spatial reasoning to extrapolate to a new scenario. I asked the new GPT-4o a variant of this problem in text:</p> <p>I set a plate on a table, and glass next to it. I set a marble on the plate. Then I pick up the marble, drop it in the glass. Then I turn the glass upside down and set it on the plate. Then, I pick up the glass and put it in the microwave. Where is the marble?</p> <p>I've added an extra item, a plate, for just a bit more complexity. Here's the answer it gave me:</p> <p>The marble is in the microwave, inside the glass.</p> <p>Here's a breakdown of what happened:</p> <p>You placed a marble on the plate.</p> <p>You picked up the marble and dropped it in the glass \u2014 now the marble is in the glass.</p> <p>You turned the glass upside down and set it on the plate \u2014 unless the glass is sealed or the marble is somehow stuck, the marble would fall out onto the plate.</p> <p>Then you picked up the glass (now empty) and put it in the microwave.</p> <p>So at this point:</p> <p>The glass is in the microwave.</p> <p>The marble is on the plate, outside the microwave.</p> <p>But you asked: Where is the marble? You didn't say you picked it up again, so based on what you said, the marble is on the plate, not in the microwave.</p> <p>Want to run it back or add any twists?</p> <p>It's actually a pretty weird answer. It immediately answers \u2013 incorrectly \u2013 then performs post-hoc chain-of-thought reasoning and comes to the correct answer but ignores the fact that it completely contradicted itself.</p> <p>Let's now do a vision-enhanced version of this chain-of-thought reasoning and demonstrate how spatial reasoning will improve the model's reasoning capabilities.</p> Problem Statement Image <p>I prompted it with the following (including extra requirements to think visually):</p> <p>I'm going to pick up the marble, put it in the glass, turn the glass over onto the plate, pick the glass up, and then put the glass in the microwave. </p> <p>The question is, where is the marble?</p> <p>Solve this one step at a time using images to imagine what happens at each step. For each step, first state the step, and then generate an image corresponding to what the scene will look like after that step. And then summarize the image in English, highlighting the important aspects for solving the problem.</p> <p>Here are the reasoning responses from the model:</p> <p>Step 1: Pick up the marble.</p> <p>In the image, the marble has not yet been moved, but we are about to pick it up and place it in the glass.</p> <p>Step 2: Put the marble in the glass.</p> <p>In the image, the marble is now inside the glass, and the plate is empty.</p> <p>Step 3: Turn the glass over onto the plate.</p> <p>In the image, the glass is now upside down on the plate, trapping the marble inside.</p> <p>Step 4: Pick the glass up.</p> <p>In the image, the hand is lifting the glass off the plate. The marble remains on the plate, as it is no longer contained.</p> <p>Step 5: Put the glass in the microwave.</p> <p>In the image, the glass is now inside the microwave with the marble at the bottom.</p> <p>Final answer: The marble is inside the microwave, at the bottom of the glass.</p> <p>So... uh... notably, we made some amazingly consistent images, and then completely got the answer wrong. And we were so close! At Step 4, the model acknowledged that the marble was no longer contained in the glass. Unfortunately, by the time the glass was in the microwave, the model imagined that the marble had somehow followed it.</p>"},{"location":"blog/2025/03/31/visual-reasoning-is-coming-soon/#training-models-to-think-visually","title":"Training Models to Think Visually","text":"<p>I think the failure to correctly use visual reasoning is because the model hasn't yet been trained for visual reasoning.</p> <p>Who knows how OpenAI trained these models, but based on their debuting post, the training to date is almost certainly unrelated to visual reasoning. The examples in that post \u2013 while extremely impressive \u2013 are about better rendering for text, and better instruction following. But all the instructions follow examples related to image manipulation rather than reasoning. Instead, we need to start training models specifically to perform visual reasoning.</p> <p>For visual reasoning practice, we can do supervised fine-tuning on sequences similar to the marble example above. For instance, to understand more about the physical world, we can show the model sequential pictures of Slinkys going down stairs, or basketball players shooting 3-pointers, or people hammering birdhouses together. We can make the model become more socially aware by letting it see sequences of text and images from social interactions, and having it predict body language and facial expressions for the next scene. We can train for spatial reasoning by having models solve tangrams or having them visualize what a contraption will look like when rotated by 90 degrees. If you think for very long it's easy to can come up with tons of ideas like these \u2013 visual reasoning is central to most of our lives.</p> <p>But where will we get all this training data? For spatial and physical reasoning tasks, we can leverage computer graphics to generate synthetic data. This approach is particularly valuable because simulations provide a controlled environment where we can create scenarios with known outcomes, making it easy to verify the model's predictions. But we'll also need real-world examples. Fortunately, there's an abundance of video content online that we can tap into. While initial datasets might require human annotation, soon models themselves will be able to process videos and their transcripts to extract training examples automatically. Consider all the how-to videos online - creators typically explain what they're going to do before demonstrating it. We can use these to create training pairs: given the current scene and verbal description, have models predict what happens next.</p> <p>As an aside \u2013 I think that videos are going to be our richest source of new training data, and we're just now starting to really tap into it. But who owns the all of the video content on the web? YouTube... Google. I'd say that bodes really well for the future of the Gemini models. Wouldn't you?</p>"},{"location":"blog/2025/03/31/visual-reasoning-is-coming-soon/#from-chain-of-thought-to-reasoning-models","title":"From Chain-of-Thought to Reasoning Models","text":"<p>As early as 2022 we had recognized that LLMs profited from chain-of-thought reasoning. If a model was making a snap judgment and rushing to an incorrect solution, it was commonplace to add a \"let's think step by step\", hope for a better output, and often get it. Soon, deeper reasoning approaches were discovered. In particular, Tree of Thoughts encouraged the model to think through multiple possible approaches for solving a problem, rank them according to perceived feasibility, and then the application would help direct the model optimally through the paths and toward the problem solution.</p> <p>With OpenAI's o1 model and several models that followed soon thereafter, the ability to do this sort of tree-of-thoughts reasoning is baked into the model. Prior to providing a final solution, the model may now think through the problem within special \"thinking\" tags. According to DeepSeek (who has been much more transparent than OpenAI), their R1 model has been trained through a form of reinforcement learning with verifiable problems so that the model can be rewarded for efficiently thinking its way to the verifiable correct solution. Interestingly, and reminiscent of tree-of-thoughts, the models can think themselves into a corner \u2013 recognize that they have messed up \u2013 backtrack, and get back to a better course toward solving the problem.</p> <p>Enter visual reasoning. In the coming year, models will undergo supervised fine-tuning to develop scene prediction abilities - given a current scene and a proposed change, they'll learn to visualize the outcome. For example, they'll be able to mentally rotate objects in 3D space or anticipate how people might react in social situations. As training progresses using scenarios with verifiable outcomes, models will develop more sophisticated reasoning capabilities. They'll be able to observe their environment, formulate plans, mentally simulate the consequences of different actions, and learn from comparing their predictions to real-world results. This advancement will be particularly transformative for robotics, where physical interaction with the environment requires robust visual understanding and planning.</p> <p>Currently, there is one obvious problem with visual reasoning \u2013 image creation is a very slow process. However, like everything else in our world right now, this will soon become faster and more feasible. But even if the visual processing remains slow, the very act of training the models to reason about images will improve the text reasoning for visual/spatial problems. What's more, the internal representation of the images seems to be created at several levels of granularity \u2013 a small blurry image, and then a refined high-def image. The image you see when playing with ChatGPT is the latter, high-def image. Perhaps visual reasoning can make use of the blurry image \u2013 it will be quicker to generate and will help with the reasoning process.</p>"},{"location":"blog/2025/03/31/visual-reasoning-is-coming-soon/#conclusion","title":"Conclusion","text":"<p>I'm excited about what's coming! Just like every moment for the past 4 years, we're on the cusp of a radical discovery that will change our world \u2013 a radical discovery which is soon realized. I don't think this one is going to let us down. The ever-more capable visual reasoning models will be able to make better sense of our work \u2013 not only in terms of understanding the mechanics of physical objects, but also in reading social cues, and really in anything else that we do where vision is of use to us! And then 2026 will be the year of the robots...</p>"},{"location":"blog/2025/03/31/visual-reasoning-is-coming-soon/#update-november-4th-2025","title":"Update November 4th, 2025","text":"<p>I just got access to Sora 2 for video generation, and it's spectacular! So \u2013 hopeful \u2013 I attempted to recreate my broken experiment above to see if visual reasoning was now the state of the art ... and ... maybe not quite yet. </p> <p>Here are my two attempts (both of which also included the first picture above of the plate, the glass, and the marble):</p> <p>I'm going to pick up the marble, put it in the glass, turn the glass over onto the plate, pick the glass up, and then put the glass in the microwave. The question is, where is the marble? Perform of each of these steps and state aloud what you\u2019re doing each time and then when you\u2019re finished, tell us exactly where the marble is.</p> <p>This lead to this video.</p> <p>After this I tried a more careful prompt - which lead to worse results.</p> <p>For the following experiment, you have a plate, a glass, and a marble. Perform the following the experiment, in a single take with no cuts: Pick up the marble and drop it in the glass. In one motion turn the glass over onto the plate upside down. Then pick the glass up, and then put the glass in the microwave. As you are doing all of this narrate each step out loud. The question is, where is the marble? Once you are finished with the above steps, tell the viewers where the marble is. And then pan and zoom the camera to focus upon the marble.</p> <p>This prompt lead to this generated video.</p> <p>In both cases the request was disobeyed, and the objects referred to verbally by the narrator were not present on the screen! Curiously though, in both videos, the narrator reached the same correct conclusion despite none of video making sense. I wonder how many times the AI would reach the correct conclusion even though the video didn't portray it. This might indicate that there is some sort of semantic reasoning happening in the \"old\" LLM portion of the model, and that reasoning is being conveyed into the video (because it at least is attempting to follow the process as I requested). But then the information from the video doesn't seem as tightly bound to the semantics. It's consistent with the initial prompt, and it's mostly self-consistent, but the semantics portions of the model don't appear to be noticing the facts of what it \"sees with its eyes\".</p> <p>Nevertheless, I'm still going to keep an eye on this space. I think over the next year or two, we're going to see AI become \"embodied\" into physical system that work with us, and this kind of visual pre-simulation is going to be important for helping them to navigate their world. I suspect it won't be visual in the exact same sense as generating a high-quality video, but it will be something analogous to this, perhaps in a lower resolution space or even a more abstract embedding space that still captures the spatially salient aspects. And I think we'll do well to train the models to attend more to the visuals that they are producing so that they remain consistent and moored to the semantics, and actually inform the semantic layer \u2013 after all, that's what visual reasoning is.</p>"},{"location":"blog/2025/03/31/visual-reasoning-is-coming-soon/#other-updates","title":"Other Updates","text":"<ul> <li>2025.11.11 - From Words to Worlds: Spatial Intelligence is AI\u2019s Next Frontier \u2013 Fei-Fei Li's essay on the importance of spatial reasoning and how to get there.</li> </ul>"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/agentic-ai/","title":"Agentic AI","text":""},{"location":"blog/category/prompt-engineering/","title":"Prompt Engineering","text":""},{"location":"blog/category/user-experience/","title":"User Experience","text":""},{"location":"blog/category/e-commerce/","title":"E-commerce","text":""},{"location":"blog/category/development-methodology/","title":"Development Methodology","text":""},{"location":"blog/category/automation/","title":"Automation","text":""},{"location":"blog/category/vibe-coding/","title":"Vibe Coding","text":""},{"location":"blog/category/coding-patterns/","title":"Coding Patterns","text":""},{"location":"blog/category/classification/","title":"Classification","text":""},{"location":"blog/category/reasoning/","title":"Reasoning","text":""},{"location":"blog/category/multimodal/","title":"Multimodal","text":""},{"location":"blog/category/retrieval/","title":"Retrieval","text":""},{"location":"blog/page/2/","title":"Blog","text":""}]}